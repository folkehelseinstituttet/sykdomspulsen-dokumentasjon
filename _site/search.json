{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\nDevelopment\ntest test test\nThis package is developed by researchers at FHI.(TO BE FILLED IN)\nHow to contact us\nIf you have any questions about our privacy policy, or you would like to exercise one of your data protection rights, please do not hesitate to contact us.\nEmail us at: sykdomspulsen@fhi.no\nPrivacy\nThis privacy policy will explain how Sykdomspulsen uses the personal data we collect from you when you use our website for technical documentation (sykdomspulsen-dokumentasjon.no).\nWhat data do we collect?\nOn sykdomspulsen-dokumentasjon.no we do not collect any information.\nCookies\nCookies are text files placed on your computer to collect standard Internet log information and visitor behavior information. For further information, visit allaboutcookies.org.\nHow do we use cookies?\nWe do not use cookies.\nChanges to our privacy policy\nWe keep our privacy policy under regular review and places any updates on this web page. This privacy policy was last updated on 2020-02-25\nHow to contact the appropriate authority\nShould you wish to report a complaint or if you feel that we have not addressed your concern in a satisfactory manner, you may contact datatilsynet at https://www.datatilsynet.no/om-datatilsynet/kontakt-oss/\n\n\n\n",
      "last_modified": "2021-07-13T06:57:09+02:00"
    },
    {
      "path": "analytics_tips_and_tricks.html",
      "title": "Tips and Tricks",
      "description": "Notes on how to use Sykdomspulsen Analytics infrastructure\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-06-02",
      "contents": "\n\nContents\nFAQ\nData, argset, schema\nRunning tasks\nChangelog\n\nFAQ\nData, argset, schema\nIt is necessary to know which analysis you are working on. All the analyses within a task can be accessed by sc::tm_get_plans_argsets_as_dt().\n\n\noptions(width = 150)\n# if don't know which plan to choose, can examine by \nsc::tm_get_plans_argsets_as_dt('ui_autoc19_report_county')\n\n\n    index_plan index_analysis location_code                                                               folder\n 1:          1              1      county03 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 2:          2              1      county11 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 3:          3              1      county15 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 4:          4              1      county18 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 5:          5              1      county30 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 6:          6              1      county34 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 7:          7              1      county38 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 8:          8              1      county42 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n 9:          9              1      county46 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n10:         10              1      county50 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n11:         11              1      county54 sykdomspulsen_covid19_autoreport_kommune_output/{argset$today}/fylke\n                                        file    week index      today first_analysis first_argset last_analysis last_argset\n 1: _dagsrapport_covid19_{argset$today}.docx 2021-28     1 2021-07-13           TRUE         TRUE         FALSE       FALSE\n 2: _dagsrapport_covid19_{argset$today}.docx 2021-28     2 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 3: _dagsrapport_covid19_{argset$today}.docx 2021-28     3 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 4: _dagsrapport_covid19_{argset$today}.docx 2021-28     4 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 5: _dagsrapport_covid19_{argset$today}.docx 2021-28     5 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 6: _dagsrapport_covid19_{argset$today}.docx 2021-28     6 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 7: _dagsrapport_covid19_{argset$today}.docx 2021-28     7 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 8: _dagsrapport_covid19_{argset$today}.docx 2021-28     8 2021-07-13          FALSE        FALSE         FALSE       FALSE\n 9: _dagsrapport_covid19_{argset$today}.docx 2021-28     9 2021-07-13          FALSE        FALSE         FALSE       FALSE\n10: _dagsrapport_covid19_{argset$today}.docx 2021-28    10 2021-07-13          FALSE        FALSE         FALSE       FALSE\n11: _dagsrapport_covid19_{argset$today}.docx 2021-28    11 2021-07-13          FALSE        FALSE          TRUE        TRUE\n\nRunning tasks\nRun externally (without manually loading data into environment)\nThe analysis (main function) is run in loops, with data/argset/schema iterating over all combinations.\nThe following two are equivalent:\n\n\ntm_run_task('analysis_covid_metrics')\n\n\n\nWhat it actually does:\n\n\nretval <- list()\n\n# 1:5 is an example\nfor(index_plan in 1:5){  \n  # data is the same for each plan\n  data <- plans[[index_plan]]$get_data()\n  \n  # argset could be different for the SAME DATA\n  for(index_analysis in 1:5){\n    argset <- plans[[index_plan]]$get_argset(index_analysis)\n    \n    # then run the MAIN ANALYSIS \n    retval[[index]] <- analysis_covid_metrics(data = data, argset = argset) # schema = schema\n  }\n}\n\n\n\nRun internally (manually)\nSince we do not run with the loop above, it is necessary to specify ONE set of data/argset/schema manually. This is done with the following.\n\n\nif(plnr::is_run_directly()){\n  # sc::tm_get_plans_argsets_as_dt(\"skuhr_import_data_recent\")\n\n  index_plan <- 7\n  index_analysis <- 1\n\n  data <- sc::tm_get_data(\"skuhr_import_data_recent\", index_plan = index_plan)\n  argset <- sc::tm_get_argset(\"skuhr_import_data_recent\", index_plan = index_plan, index_analysis = index_analysis)\n  schema <- sc::tm_get_schema(\"skuhr_import_data_recent\")\n}\n\n\n\nAs this chunk is INSIDE the main function, it overwrites the argument values.\nChangelog\n2021-06-02: Originally published.\n\n\n\n",
      "last_modified": "2021-07-13T06:57:25+02:00"
    },
    {
      "path": "concepts_db_schemas.html",
      "title": "DB Schemas",
      "description": "How do database tables work in Sykdomspulsen Core?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-05-26",
      "contents": "\n\nContents\nIntroduction\nDatabase servers\nAccess level (anon/restr/redirect/daar)\nCreating your own\nLoading data into a db schema\nAccessing the data in a db schema\nAccessing the data in ad-hoc analyses\nChangelog\n\nIntroduction\nA database schema is our way of representing how the database is constructed. In short, you can think of these as database tables.\nDatabase servers\nCurrently we have two database servers that run parallel systems. One database server is dm-prod (“auto”) and the other is dm-test (“interactive”).\nIf you run code in RStudio Server Pro or on Airflow interactive, you will automatically be connected to the interactive database server. If you run code on Airflow auto, you will automatically be connected to the auto database server.\nAccess level (anon/restr/redirect/daar)\nWithin each database server, there are multiple databases with different access levels and censoring requirements.\nCensoring is performed via the db schema.\nanon\nThe “anonymous” database contains data that is anonymous. All Sykdomspulsen team members have access to this database.\nrestr\nThe “restricted” database contains data that is:\nIndirectly identifiable\nAnonymous\nOnly a restricted number of Sykdomspulsen team members will have access to this database.\nredirect\nThis is not technically a database, however, it is treated as one.\nIf a person creates a db schema that exists in both the anonymous and restricted databases, then Sykdomspulsen Core will automatically detect the highest level of access and connect to that database when working with redirect schemas.\n\n\n\ndaar, etc\nSome projects require specific databases that are restricted to a few employees for reasons of data security.\nCreating your own\nSykdomspulsen Core requires a lot of boilerplate code. It is strongly recommended that you use the RStudio Addins menu to help you quickly insert code templates.\n\n\n\nWe will generate three database schemas:\nrestr_example (specified via name_access)\nanon_example (specified via name_access)\nredirect_example (automatically created when both restr and anon are used)\n\n\nsc::add_schema_v8(\n  name_access = c(\"restr\", \"anon\"),\n  name_grouping = \"example\",\n  name_variant = NULL,\n  db_configs = sc::config$db_configs,\n  field_types =  c(\n    \"granularity_time\" = \"TEXT\",\n    \"granularity_geo\" = \"TEXT\",\n    \"country_iso3\" = \"TEXT\",\n    \"location_code\" = \"TEXT\",\n    \"border\" = \"INTEGER\",\n    \"age\" = \"TEXT\",\n    \"sex\" = \"TEXT\",\n    \n    \"date\" = \"DATE\",\n    \n    \"isoyear\" = \"INTEGER\",\n    \"isoweek\" = \"INTEGER\",\n    \"isoyearweek\" = \"TEXT\",\n    \"season\" = \"TEXT\",\n    \"seasonweek\" = \"DOUBLE\",\n    \n    \"calyear\" = \"INTEGER\",\n    \"calmonth\" = \"INTEGER\",\n    \"calyearmonth\" = \"TEXT\",\n\n    \"value_n\" = \"INTEGER\"\n  ),\n  keys = c(\n    \"granularity_time\",\n    \"location_code\",\n    \"date\",\n    \"age\",\n    \"sex\"\n  ),\n  censors = list(\n    restr = list(\n      value_n = sc::censor_function_factory_nothing(\"value_n\")\n    ),\n    anon = list(\n      value_n = sc::censor_function_factory_values_0_4(\"value_n\")\n    )\n  ),\n  validator_field_types = sc::validator_field_types_sykdomspulsen,\n  validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n  info = \"This db table is used for...\"\n)\n\n\n\nThis schema has a few main parts.\nNaming\nThe db schemas and tables will be given the names: name_access_name_grouping_name_variant\nIn this example, there will be three db schemas:\nrestr_example (accessible at sc::config$schemas$restr_example)\nanon_example (accessible at sc::config$schemas$anon_example)\nredirect_example (accessible at sc::config$schemas$redirect_example)\nCorresponding to two db tables:\nrestr_example\nanon_example\nname_access\nEither restr or anon\nname_grouping\nA descriptive name\nname_variant\nA descriptive name\ndb_configs\nA list that contains information about the database:\n\n\nnames(sc::config$db_configs)\n\n\n[1] \"restr\"  \"anon\"   \"config\"\n\ndb_field_types\nA vector containing the names and variable types of the columns of the database table.\nIn the vast majority of cases, the first 16 columns are standardized and will always be the same.\nPermitted variable types are:\nTEXT\nDOUBLE\nINTEGER\nBOOLEAN\nDATE\nDATETIME\nkeys\nThe columns that will form the primary key of the database table (i.e. identify unique rows).\ncensors\nvalidator_field_types\nA validator that is useful for ensuring that your database table names are consistent with predetermined rules. For example, in Sykdomspulsen we have decided that we always want the first 16 columns to be:\ngranularity_time\ngranularity_geo\ncountry_iso3\nlocation_code\nborder\nage\nsex\ndate\nisoyear\nisoweek\nisoyearweek\nseason\nseasonweek\ncalyear\ncalmonth\ncalyearmonth\nWhile developing new code we found that it was difficult to force all developers to remember to include these 16 columns in the correct order. The validator sc::validator_field_types_sykdomspulsen ensures that the first 16 columns are as expected, and otherwise the developer will not be able to run their code.\nvalidator_field_contents is a validator that ensures that the contents of your data is correct. We experienced that there were issues with granularity_time sometimes containing the value week and sometimes containing the value weekly. To maintain consistency in our data, the validator sc::validator_field_contents_sykdomspulsen will throw an error if it observes non-accepted values for certain variables.\nLoading data into a db schema\nChecklist:\nRemember that “keys” (as defined in sc::add_schema_v8) defines the uniquely identifying rows of data that are allowed in the db table\nUse sc::fill_in_missing_v8(d)\nChoose your method of loading the data (upsert/insert/drop_all_rows_and_then_upsert_data)\n\n\nsc::drop_table(\"restr_example\")\nsc::drop_table(\"anon_example\")\n\n\n\nWe check to see what schemas are available:\n\n\nstringr::str_subset(names(sc::config$schemas), \"_example$\")\n\n\n[1] \"restr_example\"    \"anon_example\"     \"redirect_example\"\n\nWe then create a fictional dataset and work with it.\n\nRemember that “keys” (as defined in sc::add_schema_v8) defines the uniquely identifying rows of data that are allowed in the db table!\n\n\noptions(width = 150)\n# fictional dataset\nd <- data.table(\n  granularity_time = \"day\",\n  granularity_geo = \"nation\",\n  country_iso3 = \"nor\",\n  location_code = \"norge\",\n  border = 2020,\n  age = \"total\",\n  sex = \"total\",\n  \n  date = c(as.Date(\"1990-01-07\"),as.Date(\"1990-01-08\")),\n  \n  isoyear = 1990,\n  isoweek = 1,\n  isoyearweek = \"1990-01\",\n  season = \"1990/1991\",\n  seasonweek = 24,\n  \n  calyear = NA,\n  calmonth = NA,\n  calyearmonth = NA,\n  \n  value_n = c(3,6)\n)\n\n# display the raw data\nd[]\n\n\n   granularity_time granularity_geo country_iso3 location_code border   age   sex       date isoyear isoweek isoyearweek    season seasonweek calyear\n1:              day          nation          nor         norge   2020 total total 1990-01-07    1990       1     1990-01 1990/1991         24      NA\n2:              day          nation          nor         norge   2020 total total 1990-01-08    1990       1     1990-01 1990/1991         24      NA\n   calmonth calyearmonth value_n\n1:       NA           NA       3\n2:       NA           NA       6\n\n\n# always fill in missing data!\nsc::fill_in_missing_v8(d)\n\n# we have three options to get the data into the db table\n# remember that \"keys\" defines the uniquely identifying rows of data that are allowed in the db table!\n# - upsert means \"update if data exists, otherwise append\"\n# - insert means \"append\" (data cannot already exist)\n\nsc::config$schemas$redirect_example$upsert_data(d)\n#sc::config$schemas$redirect_example$insert_data(d)\n#sc::config$schemas$redirect_example$drop_all_rows_and_then_upsert_data(d)\n\n\n\nAccessing the data in a db schema\nChecklist:\nsc::mandatory_db_filter\ndplyr::select\nWe extract data from db schemas using dplyr with a dbplyr backend.\n\n\noptions(width = 150)\nsc::config$schemas$redirect_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\nWe can observe the effects of censoring as defined in sc::add_schema_v8\n\n\noptions(width = 150)\nsc::config$schemas$restr_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\n\nsc::config$schemas$anon_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       0             TRUE\n2:              day         norge 1990-01-08       6            FALSE\n\nAccessing the data in ad-hoc analyses\nWhen doing ad-hoc analyses, you may access the database tables via the helper function sc::tbl\nIT IS STRICTLY FORBIDDEN TO USE THIS INSIDE SYKDOMSPULSEN TASKS!!!\nThis is because sc::tbl:\nis NOT SAFE to use in parallel programming\nbypasses the input/output control mechanisms that we apply in sc::task_from_config_v8\n\n\noptions(width = 150)\nsc::tbl(\"restr_example\") %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>% \n  as.data.table() %>% \n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\nChangelog\n2021-05-26: Originally published.\n2021-05-25: Draft created.\n\n\n\n",
      "last_modified": "2021-07-13T06:57:32+02:00"
    },
    {
      "path": "concepts_tasks.html",
      "title": "Tasks",
      "description": "What are tasks, and how do they work in Sykdomspulsen Core?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-05-26",
      "contents": "\n\nContents\nIntroduction\nDefinitions\nGeneral tasks\nPutting it together\nWeather example\naction_fn\nRun the task\nExamples of different types of tasks\nChangelog\n\nIntroduction\nA task is the basic operational unit of Sykdomspulsen Core. It is based on plnr.\nIn short, you can think of a Sykdomspulsen Core task as multiple plnr plans plus Sykdomspulsen Core db schemas.\nDefinitions\n\n\nObject\n\n\nDescription\n\n\nargset\n\n\nA named list containing arguments.\n\n\nplnr analysis\n\n\nThese are the fundamental units that are scheduled in plnr:\n\n1 argset\n\n\n1 function that takes two (or more) arguments:\n\ndata (named list)\n\n\nargset (named list)\n\n\n… (optional arguments)\n\n\n\n\ndata_selector_fn\n\n\nA function that takes two arguments:\n\nargset (named list)\n\n\nschema (named list)\n\n\nThis function provides a named list to be used as the data argument to action_fn\n\n\naction_fn\n\n\nA function that takes three arguments:\n\ndata (named list, returned from data_selector_fn)\n\n\nargset (named list)\n\n\nschema (named list)\n\n\nThis is the thing that ‘does stuff’ in Sykdomspulsen Core.\n\n\nsc analysis\n\n\nA sc analysis is essentially a plnr analysis with database schemas:\n\n1 argset\n\n\n1 action_fn\n\n\n\nplan\n\n\n\n1 data-pull (using data_selector_fn)\n\n\n1 list of sc analyses\n\n\n\ntask\n\n\nThis is is the unit that Airflow schedules.\n\n1 list of plans\n\n\nWe sometimes run the list of plans in parallel.\n\n\nGeneral tasks\n\n\n\nFigure 1: A general task showing the many options of a task.\n\n\n\nFigure 1 shows us the full potential of a task.\nData can be read from any sources, then within a plan the data will be extracted once by data_selector_fn (i.e. “one data-pull”). The data will then be provided to each analysis, which will run action_fn on:\nThe provided data\nThe provided argset\nThe provided schemas\nThe action_fn can then:\nWrite data/results to db schemas\nSend emails\nExport graphs, excel files, reports, or other physical files\nTypically only a subset of this would be done in a single task.\nPlan-heavy or analysis-heavy tasks?\nA plan-heavy task is one that has many plans and a few analyses per plan.\nAn analysis-heavy task is one that has few plans and many analyses per plan.\nIn general, a data-pull is slow and wastes time. This means that it is preferable to reduce the number of data-pulls performed by having each data-pull extract larger quantities of data. The analysis can then subset the data as required (identifed via argsets). i.e. If possible, an analysis-heavy task is preferable because it will be faster (at the cost of needing more RAM).\nObviously, if a plan’s data-pull is larger, it will use more RAM. If you need to conserve RAM, then you should use a plan-heavy approach.\nFigure 1 shows only 2 location based analyses, but in reality there are 356 municipalities in Norway in 2021. If figure 1 had 2 plans (1 for 2021 data, 1 for 2020 data) and 356 analyses for each plan (1 for each location_code) then we would be taking an analysis-heavy approach.\nPutting it together\n\n\n\nFigure 2: A typical file setup for an implementation of Sykdomspulsen Core. plan_argset_fn is rarely used, and is therefore shown as blacked out in the most of the tasks.\n\n\n\nFigure 2 shows a typical implementation of Sykdomspulsen Core.\nconfig_db.r contains all of the Sykdomspulsen Core db schemas definitions. i.e. A long list of sc::add_schema_v8 commands.\nconfig_tasks.r contains all of the task definitions. i.e. A long list of sc::add_task_from_config_v8 commands.\nThen we have a one file for each task that contains the action_fn, data_selector_fn and other functions that are relevant to the task at hand.\nWeather example\nWe will now go through an example of how a person would design and implement tasks relating to weather\ndb schema\nAs documented in more detail here, we create a db schema that fits our needs (recording weather data).\n\n\n\n\n\nsc::add_schema_v8(\n  name_access = c(\"anon\"),\n  name_grouping = \"example_weather\",\n  name_variant = NULL,\n  db_configs = sc::config$db_configs,\n  field_types =  c(\n    \"granularity_time\" = \"TEXT\",\n    \"granularity_geo\" = \"TEXT\",\n    \"country_iso3\" = \"TEXT\",\n    \"location_code\" = \"TEXT\",\n    \"border\" = \"INTEGER\",\n    \"age\" = \"TEXT\",\n    \"sex\" = \"TEXT\",\n    \n    \"date\" = \"DATE\",\n    \n    \"isoyear\" = \"INTEGER\",\n    \"isoweek\" = \"INTEGER\",\n    \"isoyearweek\" = \"TEXT\",\n    \"season\" = \"TEXT\",\n    \"seasonweek\" = \"DOUBLE\",\n    \n    \"calyear\" = \"INTEGER\",\n    \"calmonth\" = \"INTEGER\",\n    \"calyearmonth\" = \"TEXT\",\n\n    \"tg\" = \"DOUBLE\",\n    \"tx\" = \"DOUBLE\",\n    \"tn\" = \"DOUBLE\",\n    \"rr\" = \"DOUBLE\"\n  ),\n  keys = c(\n    \"granularity_time\",\n    \"location_code\",\n    \"date\",\n    \"age\",\n    \"sex\"\n  ),\n  censors = list(\n    anon = list(\n      \n    )\n  ),\n  validator_field_types = sc::validator_field_types_sykdomspulsen,\n  validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n  info = \"This db table is used for...\"\n)\n\n\n\ntask_from_config_v8\nTo “register” our task, we use the RStudio addin task_from_config.\n\n\n\n\n\n# tm_run_task(\"example_weather_import_data_from_api\")\nsc::add_task_from_config_v8(\n  name_grouping = \"example_weather\",\n  name_action = \"import_data_from_api\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, # \"PACKAGE::TASK_NAME_plan_analysis\"\n  for_each_plan = plnr::expand_list(\n    location_code = \"county03\" # fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = NULL,\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_weather_import_data_from_api_action\",\n  data_selector_fn_name = \"example_weather_import_data_from_api_data_selector\",\n  schema = list(\n    # input\n\n    # output\n    \"anon_example_weather\" = sc::config$schemas$anon_example_weather\n  ),\n  info = \"This task does...\"\n)\n\n\n\nThere are a number of important things in this code that need highlighting.\nfor_each_plan\nfor_each_plan expects a list. Each component of the list will correspond to a plan, with the values added to the argset of all the analyses inside the plan.\nFor example, the following code would give 4 plans, with 1 analysis per each plan, with each analysis containing argset$var_1 and argset$var_2 as appropriate.\n\n\nfor_each_plan <- list()\nfor_each_plan[[1]] <- list(\n  var_1 = 1,\n  var_2 = \"a\"\n)\nfor_each_plan[[2]] <- list(\n  var_1 = 2,\n  var_2 = \"b\"\n)\nfor_each_plan[[3]] <- list(\n  var_1 = 1,\n  var_2 = \"a\"\n)\nfor_each_plan[[4]] <- list(\n  var_1 = 2,\n  var_2 = \"b\"\n)\n\n\n\nYou always need at least 1 plan. The most simple plan possible is:\n\n\nplnr::expand_list(\n  x = 1\n)\n\n\n[[1]]\n[[1]]$x\n[1] 1\n\nplnr::expand_list\nplnr::expand_list is esentially the same as expand.grid, except that its return values are lists instead of data.frame.\nThe code above could be simplified as follows.\n\n\nfor_each_plan <- plnr::expand_list(\n  var_1 = c(1,2),\n  var_2 = c(\"a\", \"b\")\n)\nfor_each_plan\n\n\n[[1]]\n[[1]]$var_1\n[1] 1\n\n[[1]]$var_2\n[1] \"a\"\n\n\n[[2]]\n[[2]]$var_1\n[1] 2\n\n[[2]]$var_2\n[1] \"a\"\n\n\n[[3]]\n[[3]]$var_1\n[1] 1\n\n[[3]]$var_2\n[1] \"b\"\n\n\n[[4]]\n[[4]]$var_1\n[1] 2\n\n[[4]]$var_2\n[1] \"b\"\n\nfor_each_analysis\nfor_each_plan expects a list, which will generate length(for_each_plan) plans.\nfor_each_analysis is the same, except it will generate analyses within each of the plans.\nuniversal_argset\nA named list that will add the values to the argset of all the analyses.\nupsert_at_end_of_each_plan\nIf TRUE and schema contains a schema called output, then the returned values of action_fn will be stored and upserted to schema$output at the end of each plan.\nIf you choose to upsert/insert manually from within action_fn, you can only do so at the end of each analysis.\ninsert_at_end_of_each_plan\nIf TRUE and schema contains a schema called output, then the returned values of action_fn will be stored and inserted to schema$output at the end of each plan.\nIf you choose to upsert/insert manually from within action_fn, you can only do so at the end of each analysis.\naction_fn_name\nA character string of the action_fn, preferably including the package name.\ndata_selector_fn_name\nA character string of the data_selector_fn, preferably including the package name.\nschema\nA named list containing the schemas used in this task.\ndata_selector_fn\nUse the addins dropdown to easily add in boilerplate code.\n\n\n\nThe data_selector_fn is used to extract the data for each plan.\nThe lines inside if(plnr::is_run_directly()){ are used to help developers. You can run the code manually/interactively to “load” the values of argset and schema.\n\n\nindex_plan <- 1\n\nargset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan)\nschema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n\nprint(argset)\n\n\n$`**universal**`\n[1] \"*\"\n\n$`**plan**`\n[1] \"*\"\n\n$location_code\n[1] \"county03\"\n\n$`**analysis**`\n[1] \"*\"\n\n$`**automatic**`\n[1] \"*\"\n\n$index\n[1] 1\n\n$today\n[1] \"2021-07-13\"\n\n$yesterday\n[1] \"2021-07-12\"\n\n$first_analysis\n[1] TRUE\n\n$first_argset\n[1] TRUE\n\n$last_analysis\n[1] TRUE\n\n$last_argset\n[1] TRUE\n\nprint(names(schema))\n\n\n[1] \"anon_example_weather\"\n\n\n\n# **** data_selector **** ----\n#' example_weather_import_data_from_api (data selector)\n#' @param argset Argset\n#' @param schema DB Schema\n#' @export\nexample_weather_import_data_from_api_data_selector = function(argset, schema){\n  if(plnr::is_run_directly()){\n    # sc::tm_get_plans_argsets_as_dt(\"example_weather_import_data_from_api\")\n\n    index_plan <- 1\n\n    argset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan)\n    schema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n  }\n\n  # find the mid lat/long for the specified location_code\n  gps <- fhimaps::norway_nuts3_map_b2020_default_dt[location_code == argset$location_code,.(\n    lat = mean(lat),\n    long = mean(long)\n  )]\n  \n  # download the forecast for the specified location_code\n  d <- httr::GET(glue::glue(\"https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}\"), httr::content_type_xml())\n  d <- xml2::read_xml(d$content)\n\n  # The variable returned must be a named list\n  retval <- list(\n    \"data\" = d\n  )\n  retval\n}\n\n\n\naction_fn\nThe lines inside if(plnr::is_run_directly()){ are used to help developers. You can run the code manually/interactively to “load” the values of argset and schema.\n\n\nindex_plan <- 1\nindex_analysis <- 1\n\ndata <- sc::tm_get_data(\"example_weather_import_data_from_api\", index_plan = index_plan)\nargset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan, index_analysis = index_analysis)\nschema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n\nprint(data)\n\n\n$data\n{xml_document}\n<weatherdata noNamespaceSchemaLocation=\"https://schema.api.met.no/schemas/weatherapi-0.4.xsd\" created=\"2021-07-13T04:39:05Z\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n[1] <meta>\\n  <model name=\"met_public_forecast\" termin=\"2021-07-13T ...\n[2] <product class=\"pointData\">\\n  <time datatype=\"forecast\" from=\" ...\n\nprint(argset)\n\n\n$`**universal**`\n[1] \"*\"\n\n$`**plan**`\n[1] \"*\"\n\n$location_code\n[1] \"county03\"\n\n$`**analysis**`\n[1] \"*\"\n\n$`**automatic**`\n[1] \"*\"\n\n$index\n[1] 1\n\n$today\n[1] \"2021-07-13\"\n\n$yesterday\n[1] \"2021-07-12\"\n\n$first_analysis\n[1] TRUE\n\n$first_argset\n[1] TRUE\n\n$last_analysis\n[1] TRUE\n\n$last_argset\n[1] TRUE\n\nprint(names(schema))\n\n\n[1] \"anon_example_weather\"\n\n\n\n# **** action **** ----\n#' example_weather_import_data_from_api (action)\n#' @param data Data\n#' @param argset Argset\n#' @param schema DB Schema\n#' @export\nexample_weather_import_data_from_api_action <- function(data, argset, schema) {\n  # tm_run_task(\"example_weather_import_data_from_api\")\n\n  if(plnr::is_run_directly()){\n    # sc::tm_get_plans_argsets_as_dt(\"example_weather_import_data_from_api\")\n\n    index_plan <- 1\n    index_analysis <- 1\n\n    data <- sc::tm_get_data(\"example_weather_import_data_from_api\", index_plan = index_plan)\n    argset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan, index_analysis = index_analysis)\n    schema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n  }\n\n  # code goes here\n  # special case that runs before everything\n  if(argset$first_analysis == TRUE){\n\n  }\n  \n  a <- data$data\n  \n  baz <- xml2::xml_find_all(a, \".//maxTemperature\")\n  res <- vector(\"list\", length = length(baz))\n  for (i in seq_along(baz)) {\n    parent <- xml2::xml_parent(baz[[i]])\n    grandparent <- xml2::xml_parent(parent)\n    time_from <- xml2::xml_attr(grandparent, \"from\")\n    time_to <- xml2::xml_attr(grandparent, \"to\")\n    x <- xml2::xml_find_all(parent, \".//minTemperature\")\n    temp_min <- xml2::xml_attr(x, \"value\")\n    x <- xml2::xml_find_all(parent, \".//maxTemperature\")\n    temp_max <- xml2::xml_attr(x, \"value\")\n    x <- xml2::xml_find_all(parent, \".//precipitation\")\n    precip <- xml2::xml_attr(x, \"value\")\n    res[[i]] <- data.frame(\n      time_from = as.character(time_from),\n      time_to = as.character(time_to),\n      tx = as.numeric(temp_max),\n      tn = as.numeric(temp_min),\n      rr = as.numeric(precip)\n    )\n  }\n  res <- rbindlist(res)\n  res <- res[stringr::str_sub(time_from, 12, 13) %in% c(\"00\", \"06\", \"12\", \"18\")]\n  res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]\n  res[, N := .N, by = date]\n  res <- res[N == 4]\n  res <- res[\n    , \n    .(\n      tg = NA,\n      tx = max(tx),\n      tn = min(tn),\n      rr = sum(rr)\n    ),\n    keyby = .(date)\n  ]\n  \n  # we look at the downloaded data\n  print(\"Data after downloading\")\n  print(res)\n  \n  # we now need to format it\n  res[, granularity_time := \"day\"]\n  res[, sex := \"total\"]\n  res[, age := \"total\"]\n  res[, location_code := argset$location_code]\n  \n  # fill in missing structural variables\n  sc::fill_in_missing_v8(res, border = 2020)\n  \n  # we look at the downloaded data\n  print(\"Data after missing structural variables filled in\")\n  print(res)\n\n  # put data in db table\n  # schema$SCHEMA_NAME$insert_data(d)\n  schema$anon_example_weather$upsert_data(res)\n  # schema$SCHEMA_NAME$drop_all_rows_and_then_upsert_data(d)\n\n  # special case that runs after everything\n  # copy to anon_web?\n  if(argset$last_analysis == TRUE){\n    # sc::copy_into_new_table_where(\n    #   table_from = \"anon_X\",\n    #   table_to = \"anon_webkht\"\n    # )\n  }\n}\n\n\n\nRun the task\n\n\ntm_run_task(\"example_weather_import_data_from_api\")\n\n\n[1] \"Data after downloading\"\n         date tg   tx   tn   rr\n1: 2021-07-14 NA 24.1 17.0 20.8\n2: 2021-07-15 NA 28.1 17.7  0.0\n3: 2021-07-16 NA 25.8 15.1  0.0\n4: 2021-07-17 NA 27.5 15.1  0.0\n5: 2021-07-18 NA 27.4 14.9  0.0\n6: 2021-07-19 NA 25.0 14.2  0.0\n7: 2021-07-20 NA 25.2 14.1  0.0\n8: 2021-07-21 NA 25.7 13.4  0.0\n[1] \"Data after missing structural variables filled in\"\n         date tg   tx   tn   rr granularity_time   sex   age\n1: 2021-07-14 NA 24.1 17.0 20.8              day total total\n2: 2021-07-15 NA 28.1 17.7  0.0              day total total\n3: 2021-07-16 NA 25.8 15.1  0.0              day total total\n4: 2021-07-17 NA 27.5 15.1  0.0              day total total\n5: 2021-07-18 NA 27.4 14.9  0.0              day total total\n6: 2021-07-19 NA 25.0 14.2  0.0              day total total\n7: 2021-07-20 NA 25.2 14.1  0.0              day total total\n8: 2021-07-21 NA 25.7 13.4  0.0              day total total\n   location_code granularity_geo border isoyearweek    season isoyear\n1:      county03          county   2020     2021-28 2020/2021    2021\n2:      county03          county   2020     2021-28 2020/2021    2021\n3:      county03          county   2020     2021-28 2020/2021    2021\n4:      county03          county   2020     2021-28 2020/2021    2021\n5:      county03          county   2020     2021-28 2020/2021    2021\n6:      county03          county   2020     2021-29 2020/2021    2021\n7:      county03          county   2020     2021-29 2020/2021    2021\n8:      county03          county   2020     2021-29 2020/2021    2021\n   isoweek seasonweek calyear calmonth calyearmonth country_iso3\n1:      28         51    2021        7     2021-M07          nor\n2:      28         51    2021        7     2021-M07          nor\n3:      28         51    2021        7     2021-M07          nor\n4:      28         51    2021        7     2021-M07          nor\n5:      28         51    2021        7     2021-M07          nor\n6:      29         52    2021        7     2021-M07          nor\n7:      29         52    2021        7     2021-M07          nor\n8:      29         52    2021        7     2021-M07          nor\n\nExamples of different types of tasks\nImporting data\n\n\nknitr::include_graphics(\"analytics_tasks_introduction/task_import_data.png\")\n\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"import_data\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL,\n  for_each_plan = plnr::expand_list(\n    x = 1\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"input\", \"example\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_import_data_action\",\n  data_selector_fn_name = \"example_import_data_data_selector\",\n  schema = list(\n    # input\n\n    # output\n    \"output\" = sc::config$schemas$output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nAnalysis\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"analysis\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = NULL,\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_analysis_action\",\n  data_selector_fn_name = \"example_analysis_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input,\n\n    # output\n    \"output\" = sc::config$schemas$output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nExporting multiple sets of results\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"export_results\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"output\", \"example\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_export_results_action\",\n  data_selector_fn_name = \"example_export_results_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input\n\n    # output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nExporting combined results\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"export_results\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    x = 1\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"output\", \"example\"),\n    granularity_geos = c(\"nation\", \"county\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_export_results_action\",\n  data_selector_fn_name = \"example_export_results_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input\n\n    # output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nChangelog\n2021-05-26: Draft created.\n\n\n\n",
      "last_modified": "2021-07-13T06:57:38+02:00"
    },
    {
      "path": "index.html",
      "title": "Sykdomspulsen Core",
      "description": "A free and open-source health surveillance system\n",
      "author": [],
      "contents": "\nSykdomspulsen core is a free and open-source health surveillance system designed and developed by the Norwegian Institute of Public Health.\nThe package is undergoing development now at https://github.com/folkehelseinstituttet/sc\nYou can subscribe to our newsletter to be notified when Sykdomspulsen Core is ready for public use.\n\n\n\n\n\n",
      "last_modified": "2021-07-13T06:57:39+02:00"
    }
  ],
  "collections": []
}
