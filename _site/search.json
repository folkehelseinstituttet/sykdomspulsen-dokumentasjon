{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\nContents\nDevelopment\nPrivacy\n\nDevelopment\ntest test test\nThis package is developed by researchers at FHI.(TO BE FILLED IN)\nHow to contact us\nIf you have any questions about our privacy policy, or you would like to exercise one of your data protection rights, please do not hesitate to contact us.\nEmail us at: sykdomspulsen@fhi.no\nPrivacy\nThis privacy policy will explain how Sykdomspulsen uses the personal data we collect from you when you use our website for technical documentation (sykdomspulsen-dokumentasjon.no).\nWhat data do we collect?\nOn sykdomspulsen-dokumentasjon.no we do not collect any information.\nCookies\nCookies are text files placed on your computer to collect standard Internet log information and visitor behavior information. For further information, visit allaboutcookies.org.\nHow do we use cookies?\nWe do not use cookies.\nChanges to our privacy policy\nWe keep our privacy policy under regular review and places any updates on this web page. This privacy policy was last updated on 2020-02-25\nHow to contact the appropriate authority\nShould you wish to report a complaint or if you feel that we have not addressed your concern in a satisfactory manner, you may contact datatilsynet at https://www.datatilsynet.no/om-datatilsynet/kontakt-oss/\n\n\n\n",
      "last_modified": "2021-07-15T11:12:59+02:00"
    },
    {
      "path": "analytics_tips_and_tricks.html",
      "title": "Tips and Tricks",
      "description": "Notes on how to use Sykdomspulsen Analytics infrastructure\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-06-02",
      "contents": "\n\nContents\nFAQ\nData, argset, schema\nRunning tasks\nChangelog\n\nFAQ\nData, argset, schema\nIt is necessary to know which analysis you are working on. All the analyses within a task can be accessed by sc::tm_get_plans_argsets_as_dt().\n\n\noptions(width = 150)\n# if don't know which plan to choose, can examine by \n#sc::tm_get_plans_argsets_as_dt('ui_autoc19_report_county')\n\n\n\nRunning tasks\nRun externally (without manually loading data into environment)\nThe analysis (main function) is run in loops, with data/argset/schema iterating over all combinations.\nThe following two are equivalent:\n\n\ntm_run_task('analysis_covid_metrics')\n\n\n\nWhat it actually does:\n\n\nretval <- list()\n\n# 1:5 is an example\nfor(index_plan in 1:5){  \n  # data is the same for each plan\n  data <- plans[[index_plan]]$get_data()\n  \n  # argset could be different for the SAME DATA\n  for(index_analysis in 1:5){\n    argset <- plans[[index_plan]]$get_argset(index_analysis)\n    \n    # then run the MAIN ANALYSIS \n    retval[[index]] <- analysis_covid_metrics(data = data, argset = argset) # schema = schema\n  }\n}\n\n\n\nRun internally (manually)\nSince we do not run with the loop above, it is necessary to specify ONE set of data/argset/schema manually. This is done with the following.\n\n\nif(plnr::is_run_directly()){\n  # sc::tm_get_plans_argsets_as_dt(\"skuhr_import_data_recent\")\n\n  index_plan <- 7\n  index_analysis <- 1\n\n  data <- sc::tm_get_data(\"skuhr_import_data_recent\", index_plan = index_plan)\n  argset <- sc::tm_get_argset(\"skuhr_import_data_recent\", index_plan = index_plan, index_analysis = index_analysis)\n  schema <- sc::tm_get_schema(\"skuhr_import_data_recent\")\n}\n\n\n\nAs this chunk is INSIDE the main function, it overwrites the argument values.\nChangelog\n2021-06-02: Originally published.\n\n\n\n",
      "last_modified": "2021-07-15T11:13:03+02:00"
    },
    {
      "path": "concepts_db_schemas.html",
      "title": "DB Schemas",
      "description": "How do database tables work in Sykdomspulsen Core?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-05-26",
      "contents": "\n\nContents\nIntroduction\nDatabase servers\nAccess level (anon/restr/redirect)\nCreating your own\nLoading data into a db schema\nAccessing the data in a db schema\nAccessing the data in ad-hoc analyses\nChangelog\n\nIntroduction\nA database schema is our way of representing how the database is constructed. In short, you can think of these as database tables.\nDatabase servers\nNormally, an implementation of Sykdomspulsen Core would have two database servers that run parallel systems. One database server is auto and the other is interactive.\nIf you run code in RStudio Workbench or on Airflow interactive, you should be automatically be connected to the interactive database server. If you run code on Airflow auto, you should be automatically be connected to the auto database server. This is something that your implementation will have to solve.\nAccess level (anon/restr/redirect)\nWithin each database server, there are multiple databases with different access levels and censoring requirements.\nCensoring is performed via the db schema.\nanon\nThe “anonymous” database contains data that is anonymous. All team members should have access to this database.\nrestr\nThe “restricted” database contains data that is:\nIndirectly identifiable\nAnonymous\nOnly a restricted number of team members should have access to this database.\nredirect\nThis is not technically a database, however, it is treated as one.\nIf a person creates a db schema that exists in both the anonymous and restricted databases, then Sykdomspulsen Core will automatically detect the highest level of access and connect to that database when working with redirect schemas.\n\n\n\nCreating your own\nSykdomspulsen Core requires a lot of boilerplate code. It is strongly recommended that you use the RStudio Addins menu to help you quickly insert code templates.\n\n\n\nWe will generate three database schemas:\nrestr_example (specified via name_access)\nanon_example (specified via name_access)\nredirect_example (automatically created when both restr and anon are used)\n\n\nsc::add_schema_v8(\n  name_access = c(\"restr\", \"anon\"),\n  name_grouping = \"example\",\n  name_variant = NULL,\n  db_configs = sc::config$db_configs,\n  field_types =  c(\n    \"granularity_time\" = \"TEXT\",\n    \"granularity_geo\" = \"TEXT\",\n    \"country_iso3\" = \"TEXT\",\n    \"location_code\" = \"TEXT\",\n    \"border\" = \"INTEGER\",\n    \"age\" = \"TEXT\",\n    \"sex\" = \"TEXT\",\n    \n    \"date\" = \"DATE\",\n    \n    \"isoyear\" = \"INTEGER\",\n    \"isoweek\" = \"INTEGER\",\n    \"isoyearweek\" = \"TEXT\",\n    \"season\" = \"TEXT\",\n    \"seasonweek\" = \"DOUBLE\",\n    \n    \"calyear\" = \"INTEGER\",\n    \"calmonth\" = \"INTEGER\",\n    \"calyearmonth\" = \"TEXT\",\n\n    \"value_n\" = \"INTEGER\"\n  ),\n  keys = c(\n    \"granularity_time\",\n    \"location_code\",\n    \"date\",\n    \"age\",\n    \"sex\"\n  ),\n  censors = list(\n    restr = list(\n      value_n = sc::censor_function_factory_nothing(\"value_n\")\n    ),\n    anon = list(\n      value_n = sc::censor_function_factory_values_0_4(\"value_n\")\n    )\n  ),\n  validator_field_types = sc::validator_field_types_sykdomspulsen,\n  validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n  info = \"This db table is used for...\"\n)\n\n\n\nThis schema has a few main parts.\nNaming\nThe db schemas and tables will be given the names: name_access_name_grouping_name_variant\nIn this example, there will be three db schemas:\nrestr_example (accessible at sc::config$schemas$restr_example)\nanon_example (accessible at sc::config$schemas$anon_example)\nredirect_example (accessible at sc::config$schemas$redirect_example)\nCorresponding to two db tables:\nrestr_example\nanon_example\nname_access\nEither restr or anon\nname_grouping\nA descriptive name\nname_variant\nA descriptive name\ndb_configs\nA list that contains information about the database:\n\n\nnames(sc::config$db_configs)\n\n\n[1] \"restr\"  \"anon\"   \"config\"\n\ndb_field_types\nA vector containing the names and variable types of the columns of the database table.\nIn the vast majority of cases, the first 16 columns are standardized and will always be the same.\nPermitted variable types are:\nTEXT\nDOUBLE\nINTEGER\nBOOLEAN\nDATE\nDATETIME\nkeys\nThe columns that will form the primary key of the database table (i.e. identify unique rows).\ncensors\nvalidator_field_types\nA validator that is useful for ensuring that your database table names are consistent with predetermined rules. For example, in Sykdomspulsen we have decided that we always want the first 16 columns to be:\ngranularity_time\ngranularity_geo\ncountry_iso3\nlocation_code\nborder\nage\nsex\ndate\nisoyear\nisoweek\nisoyearweek\nseason\nseasonweek\ncalyear\ncalmonth\ncalyearmonth\nWhile developing new code we found that it was difficult to force all developers to remember to include these 16 columns in the correct order. The validator sc::validator_field_types_sykdomspulsen ensures that the first 16 columns are as expected, and otherwise the developer will not be able to run their code.\nvalidator_field_contents is a validator that ensures that the contents of your data is correct. We experienced that there were issues with granularity_time sometimes containing the value week and sometimes containing the value weekly. To maintain consistency in our data, the validator sc::validator_field_contents_sykdomspulsen will throw an error if it observes non-accepted values for certain variables.\nLoading data into a db schema\nChecklist:\nRemember that “keys” (as defined in sc::add_schema_v8) defines the uniquely identifying rows of data that are allowed in the db table\nUse sc::fill_in_missing_v8(d)\nChoose your method of loading the data (upsert/insert/drop_all_rows_and_then_upsert_data)\n\n\n\nWe check to see what schemas are available:\n\n\nstringr::str_subset(names(sc::config$schemas), \"_example$\")\n\n\n[1] \"restr_example\"    \"anon_example\"     \"redirect_example\"\n\nWe then create a fictional dataset and work with it.\n\nRemember that “keys” (as defined in sc::add_schema_v8) defines the uniquely identifying rows of data that are allowed in the db table!\n\n\noptions(width = 150)\n# fictional dataset\nd <- data.table(\n  granularity_time = \"day\",\n  granularity_geo = \"nation\",\n  country_iso3 = \"nor\",\n  location_code = \"norge\",\n  border = 2020,\n  age = \"total\",\n  sex = \"total\",\n  \n  date = c(as.Date(\"1990-01-07\"),as.Date(\"1990-01-08\")),\n  \n  isoyear = 1990,\n  isoweek = 1,\n  isoyearweek = \"1990-01\",\n  season = \"1990/1991\",\n  seasonweek = 24,\n  \n  calyear = NA,\n  calmonth = NA,\n  calyearmonth = NA,\n  \n  value_n = c(3,6)\n)\n\n# display the raw data\nd[]\n\n\n   granularity_time granularity_geo country_iso3 location_code border   age   sex       date isoyear isoweek isoyearweek    season seasonweek calyear\n1:              day          nation          nor         norge   2020 total total 1990-01-07    1990       1     1990-01 1990/1991         24      NA\n2:              day          nation          nor         norge   2020 total total 1990-01-08    1990       1     1990-01 1990/1991         24      NA\n   calmonth calyearmonth value_n\n1:       NA           NA       3\n2:       NA           NA       6\n\n\n# always fill in missing data!\nsc::fill_in_missing_v8(d)\n\n# we have four options to get the data into the db table\n# remember that \"keys\" defines the uniquely identifying rows of data that are allowed in the db table!\n# - upsert means \"update if data exists, otherwise append\"\n# - insert means \"append\" (data cannot already exist)\n\nsc::config$schemas$redirect_example$upsert_data(d)\n#sc::config$schemas$redirect_example$insert_data(d)\n#sc::config$schemas$redirect_example$drop_all_rows_and_then_upsert_data(d)\n#sc::config$schemas$redirect_example$drop_all_rows_and_then_insert_data(d)\n\n\n\nAccessing the data in a db schema\nChecklist:\nsc::mandatory_db_filter\ndplyr::select\nWe extract data from db schemas using dplyr with a dbplyr backend.\n\n\noptions(width = 150)\nsc::config$schemas$redirect_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\nWe can observe the effects of censoring as defined in sc::add_schema_v8\n\n\noptions(width = 150)\nsc::config$schemas$restr_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\n\nsc::config$schemas$anon_example$tbl() %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>%\n  as.data.table() %>%\n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       0             TRUE\n2:              day         norge 1990-01-08       6            FALSE\n\nAccessing the data in ad-hoc analyses\nWhen doing ad-hoc analyses, you may access the database tables via the helper function sc::tbl\nIT IS STRICTLY FORBIDDEN TO USE THIS INSIDE SYKDOMSPULSEN TASKS!!!\nThis is because sc::tbl:\nis NOT SAFE to use in parallel programming\nbypasses the input/output control mechanisms that we apply in sc::task_from_config_v8\n\n\noptions(width = 150)\nsc::tbl(\"restr_example\") %>%\n  sc::mandatory_db_filter(\n    granularity_time = \"day\",\n    granularity_time_not = NULL,\n    granularity_geo = NULL,\n    granularity_geo_not = NULL,\n    country_iso3 = NULL,\n    location_code = \"norge\",\n    age = \"total\",\n    age_not = NULL,\n    sex = \"total\",\n    sex_not = NULL\n  ) %>%\n  dplyr::select(\n    granularity_time,\n    location_code,\n    date,\n    value_n,\n    value_n_censored\n  ) %>%\n  dplyr::collect() %>% \n  as.data.table() %>% \n  print()\n\n\n   granularity_time location_code       date value_n value_n_censored\n1:              day         norge 1990-01-07       3            FALSE\n2:              day         norge 1990-01-08       6            FALSE\n\nChangelog\n2021-05-26: Originally published.\n2021-05-25: Draft created.\n\n\n\n",
      "last_modified": "2021-07-15T11:13:07+02:00"
    },
    {
      "path": "concepts_file_layout.html",
      "title": "File Layout",
      "description": "Which files go where, and what is in them?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-07-14",
      "contents": "\n\nContents\nIntroduction\n00_env_and_namespace.r\n01_definitions.r\n02_permissions.r\n03_db_schemas.r\n04_tasks.r\n05_deliverables.r\n06_config.r\n07_onLoad.r\n08_onAttach.r\n99_util_*.r\nTask files\nChangelog\n\nIntroduction\nImplementing Sykdomspulsen Core requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at https://github.com/folkehelseinstituttet/scskeleton\nWe suggest that you clone this GitHub repo to your server, and then do a global find/replace on scskeleton with the name you want for your R package.\nDescriptions of the required files/functions are detailed below.\n00_env_and_namespace.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/00_env_and_namespace.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/00_env_and_namespace.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 00_env_and_namespace.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Use roxygen2 to import ggplot2, data.table, %>%, and %<>% into the namespace,\n 8 | #   because these are the most commonly used packages/functions.\n 9 | #\n10 | # PURPOSE 2:\n11 | #   Declaring our own \"tm_run_task\" inside this package, as a wrapper around\n12 | #   sc::tm_run_task.\n13 | #\n14 | #   We cannot run sc::tm_run_task directly, because we need to load all of the\n15 | #   database connections, db schemas, tasks, etc. *before* we run the task.\n16 | #   Hence, this wrapper ensures that all of this package's configs files are\n17 | #   loaded via OURPACKAGE::.onLoad() first, and then sc::tm_run_task can run.\n18 | #\n19 | # PURPOSE 3:\n20 | #   Declaration of environments that can be used globally.\n21 | #\n22 | # PURPOSE 4:\n23 | #   Fix issues/integration with other packages.\n24 | #\n25 | #   Most notably is the issue with rmarkdown, where an error is thrown when\n26 | #   rendering multiple rmarkdown documents in parallel.\n27 | #\n28 | # ******************************************************************************\n29 | # ******************************************************************************\n30 | \n31 | #' @import ggplot2\n32 | #' @import data.table\n33 | #' @importFrom magrittr %>% %<>%\n34 | 1\n35 | \n36 | #' Shortcut to run task\n37 | #'\n38 | #' This task is needed to ensure that all the definitions/db schemas/tasks/etc\n39 | #' are loaded from the package scskeleton. We cannot run sc::tm_run_task directly,\n40 | #' because we need to load all of the database connections, db schemas, tasks,\n41 | #' etc. *before* we run the task. Hence, this wrapper ensures that all of this\n42 | #' package's configs files are loaded via OURPACKAGE::.onLoad() first, and then\n43 | #' sc::tm_run_task can run.\n44 | #'\n45 | #' @param task_name Name of the task\n46 | #' @param index_plan Not used\n47 | #' @param index_analysis Not used\n48 | #' @export\n49 | tm_run_task <- function(task_name, index_plan = NULL, index_analysis = NULL) {\n50 |   sc::tm_run_task(\n51 |     task_name = task_name,\n52 |     index_plan = index_plan,\n53 |     index_analysis = index_analysis\n54 |   )\n55 | }\n56 | \n57 | #' Declaration of environments that can be used globally\n58 | #' @export config\n59 | config <- new.env()\n60 | \n61 | # https://github.com/rstudio/rmarkdown/issues/1632\n62 | # An error is thrown when rendering multiple rmarkdown documents in parallel.\n63 | clean_tmpfiles_mod <- function() {\n64 |   # message(\"Calling clean_tmpfiles_mod()\")\n65 | }\n\n01_definitions.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/01_definitions.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/01_definitions.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 01_definitions.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Set global definitions that are used throughout the package, and further\n 8 | #   (e.g. in shiny/plumber creations).\n 9 | #\n10 | #   Examples of global definitions are:\n11 | #     - Border years\n12 | #     - Age definitions\n13 | #     - Diagnosis mappings (e.g. \"R80\" = \"Influenza\")\n14 | #\n15 | # ******************************************************************************\n16 | # ******************************************************************************\n17 | \n18 | #' Set global definitions\n19 | set_definitions <- function() {\n20 | \n21 |   # Norway's last redistricting occurred 2020-01-01\n22 |   config$border <- 2020\n23 | \n24 |   # fhidata needs to know which border is in use\n25 |   # fhidata should also replace the population of 1900 with the current year,\n26 |   # because year = 1900 is shorthand for granularity_geo = \"total\".\n27 |   # This means that it is more appropriate to use the current year's population\n28 |   # for year = 1900.\n29 |   fhidata::set_config(\n30 |     border = config$border,\n31 |     use_current_year_as_1900_pop = TRUE\n32 |   )\n33 | }\n\n02_permissions.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/02_permissions.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/02_permissions.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 02_permissions.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Set permissions that can be used in this package.\n 8 | #\n 9 | # PURPOSE 2:\n10 | #   Permissions are a way of ensuring that a task only runs once per hour/day/week.\n11 | #   This can be useful when you want to be 100% sure that you don't want to spam\n12 | #   emails to your recipients.\n13 | #\n14 | # PURPOSE 3:\n15 | #   Permissions can also be used to differentiate between \"production days\" and\n16 | #   \"preliminary days\". This can be useful when you have different email lists\n17 | #   for production days (everyone) and preliminary days (a smaller group).\n18 | #\n19 | # ******************************************************************************\n20 | # ******************************************************************************\n21 | \n22 | set_permissions <- function() {\n23 |   # sc::add_permission(\n24 |   #   name = \"khtemails_send_emails\",\n25 |   #   permission = sc::Permission$new(\n26 |   #     key = \"khtemails_send_emails\",\n27 |   #     value = as.character(lubridate::today()),  # one time per day\n28 |   #     production_days = c(3) # wed, send to everyone, otherwise prelim\n29 |   #   )\n30 |   # )\n31 | }\n\n03_db_schemas.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/03_db_schemas.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/03_db_schemas.r\n\n  1 | # ******************************************************************************\n  2 | # ******************************************************************************\n  3 | #\n  4 | # 03_db_schemas.r\n  5 | #\n  6 | # PURPOSE 1:\n  7 | #   Set db schemas that are used throughout the package.\n  8 | #\n  9 | #   These are basically all of the database tables that you will be writing to,\n 10 | #   and reading from.\n 11 | #\n 12 | # ******************************************************************************\n 13 | # ******************************************************************************\n 14 | \n 15 | set_db_schemas <- function() {\n 16 |   # __________ ----\n 17 |   # Weather  ----\n 18 |   ## > anon_example_weather_rawdata ----\n 19 |   sc::add_schema_v8(\n 20 |     name_access = c(\"anon\"),\n 21 |     name_grouping = \"example_weather\",\n 22 |     name_variant = \"rawdata\",\n 23 |     db_configs = sc::config$db_configs,\n 24 |     field_types =  c(\n 25 |       \"granularity_time\" = \"TEXT\",\n 26 |       \"granularity_geo\" = \"TEXT\",\n 27 |       \"country_iso3\" = \"TEXT\",\n 28 |       \"location_code\" = \"TEXT\",\n 29 |       \"border\" = \"INTEGER\",\n 30 |       \"age\" = \"TEXT\",\n 31 |       \"sex\" = \"TEXT\",\n 32 | \n 33 |       \"date\" = \"DATE\",\n 34 | \n 35 |       \"isoyear\" = \"INTEGER\",\n 36 |       \"isoweek\" = \"INTEGER\",\n 37 |       \"isoyearweek\" = \"TEXT\",\n 38 |       \"season\" = \"TEXT\",\n 39 |       \"seasonweek\" = \"DOUBLE\",\n 40 | \n 41 |       \"calyear\" = \"INTEGER\",\n 42 |       \"calmonth\" = \"INTEGER\",\n 43 |       \"calyearmonth\" = \"TEXT\",\n 44 | \n 45 |       \"temp_max\" = \"DOUBLE\",\n 46 |       \"temp_min\" = \"DOUBLE\",\n 47 |       \"precip\" = \"DOUBLE\"\n 48 |     ),\n 49 |     keys = c(\n 50 |       \"granularity_time\",\n 51 |       \"location_code\",\n 52 |       \"date\",\n 53 |       \"age\",\n 54 |       \"sex\"\n 55 |     ),\n 56 |     censors = list(\n 57 |       anon = list(\n 58 | \n 59 |       )\n 60 |     ),\n 61 |     validator_field_types = sc::validator_field_types_sykdomspulsen,\n 62 |     validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n 63 |     info = \"This db table is used for...\"\n 64 |   )\n 65 | \n 66 |   ## > anon_example_weather_data ----\n 67 |   sc::add_schema_v8(\n 68 |     name_access = c(\"anon\"),\n 69 |     name_grouping = \"example_weather\",\n 70 |     name_variant = \"data\",\n 71 |     db_configs = sc::config$db_configs,\n 72 |     field_types =  c(\n 73 |       \"granularity_time\" = \"TEXT\",\n 74 |       \"granularity_geo\" = \"TEXT\",\n 75 |       \"country_iso3\" = \"TEXT\",\n 76 |       \"location_code\" = \"TEXT\",\n 77 |       \"border\" = \"INTEGER\",\n 78 |       \"age\" = \"TEXT\",\n 79 |       \"sex\" = \"TEXT\",\n 80 | \n 81 |       \"date\" = \"DATE\",\n 82 | \n 83 |       \"isoyear\" = \"INTEGER\",\n 84 |       \"isoweek\" = \"INTEGER\",\n 85 |       \"isoyearweek\" = \"TEXT\",\n 86 |       \"season\" = \"TEXT\",\n 87 |       \"seasonweek\" = \"DOUBLE\",\n 88 | \n 89 |       \"calyear\" = \"INTEGER\",\n 90 |       \"calmonth\" = \"INTEGER\",\n 91 |       \"calyearmonth\" = \"TEXT\",\n 92 | \n 93 |       \"temp_max\" = \"DOUBLE\",\n 94 |       \"temp_min\" = \"DOUBLE\",\n 95 |       \"precip\" = \"DOUBLE\"\n 96 |     ),\n 97 |     keys = c(\n 98 |       \"granularity_time\",\n 99 |       \"location_code\",\n100 |       \"date\",\n101 |       \"age\",\n102 |       \"sex\"\n103 |     ),\n104 |     censors = list(\n105 |       anon = list(\n106 | \n107 |       )\n108 |     ),\n109 |     validator_field_types = sc::validator_field_types_sykdomspulsen,\n110 |     validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n111 |     info = \"This db table is used for...\"\n112 |   )\n113 | }\n\n04_tasks.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/04_tasks.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/04_tasks.r\n\n  1 | # ******************************************************************************\n  2 | # ******************************************************************************\n  3 | #\n  4 | # 04_tasks.r\n  5 | #\n  6 | # PURPOSE 1:\n  7 | #   Set all the tasks that are run by the package.\n  8 | #\n  9 | #   These are basically all of the \"things\" that you want to do.\n 10 | #   E.g. Downloading data, cleaning data, importing data, analyzing data,\n 11 | #   making Excel files, making docx/pdf reports, sending emails, etc.\n 12 | #\n 13 | # ******************************************************************************\n 14 | # ******************************************************************************\n 15 | \n 16 | set_tasks <- function() {\n 17 |   # __________ ----\n 18 |   # Weather  ----\n 19 |   ## > weather_download_and_import_rawdata ----\n 20 |   # tm_run_task(\"weather_download_and_import_rawdata\")\n 21 |   sc::add_task_from_config_v8(\n 22 |     name_grouping = \"weather\",\n 23 |     name_action = \"download_and_import_rawdata\",\n 24 |     name_variant = NULL,\n 25 |     cores = 1,\n 26 |     plan_analysis_fn_name = NULL,\n 27 |     for_each_plan = plnr::expand_list(\n 28 |       location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"municip\")]$location_code\n 29 |     ),\n 30 |     for_each_analysis = NULL,\n 31 |     universal_argset = NULL,\n 32 |     upsert_at_end_of_each_plan = FALSE,\n 33 |     insert_at_end_of_each_plan = FALSE,\n 34 |     action_fn_name = \"scskeleton::weather_download_and_import_rawdata_action\",\n 35 |     data_selector_fn_name = \"scskeleton::weather_download_and_import_rawdata_data_selector\",\n 36 |     schema = list(\n 37 |       # input\n 38 | \n 39 |       # output\n 40 |       \"anon_example_weather_rawdata\" = sc::config$schemas$anon_example_weather_rawdata\n 41 |     ),\n 42 |     info = \"This task downloads and imports the raw weather data from MET's API at the municipal level\"\n 43 |   )\n 44 | \n 45 |   ## > weather_clean_data ----\n 46 |   # tm_run_task(\"weather_clean_data\")\n 47 |   sc::add_task_from_config_v8(\n 48 |     name_grouping = \"weather\",\n 49 |     name_action = \"clean_data\",\n 50 |     name_variant = NULL,\n 51 |     cores = 1,\n 52 |     plan_analysis_fn_name = NULL,\n 53 |     for_each_plan = plnr::expand_list(\n 54 |       x = 1\n 55 |     ),\n 56 |     for_each_analysis = NULL,\n 57 |     universal_argset = NULL,\n 58 |     upsert_at_end_of_each_plan = FALSE,\n 59 |     insert_at_end_of_each_plan = FALSE,\n 60 |     action_fn_name = \"scskeleton::weather_clean_data_action\",\n 61 |     data_selector_fn_name = \"scskeleton::weather_clean_data_data_selector\",\n 62 |     schema = list(\n 63 |       # input\n 64 |       \"anon_example_weather_rawdata\" = sc::config$schemas$anon_example_weather_rawdata,\n 65 | \n 66 |       # output\n 67 |       \"anon_example_weather_data\" = sc::config$schemas$anon_example_weather_data\n 68 |     ),\n 69 |     info = \"This task cleans the raw data and aggregates it to county and national level\"\n 70 |   )\n 71 | \n 72 |   ## > weather_clean_data ----\n 73 |   # tm_run_task(\"weather_export_plots\")\n 74 |   sc::add_task_from_config_v8(\n 75 |     name_grouping = \"weather\",\n 76 |     name_action = \"export_plots\",\n 77 |     name_variant = NULL,\n 78 |     cores = 1,\n 79 |     plan_analysis_fn_name = NULL,\n 80 |     for_each_plan = plnr::expand_list(\n 81 |       location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n 82 |     ),\n 83 |     for_each_analysis = NULL,\n 84 |     universal_argset = list(\n 85 |       output_dir = tempdir(),\n 86 |       output_filename = \"weather_{argset$location_code}.png\",\n 87 |       output_absolute_path = fs::path(\"{argset$output_dir}\", \"{argset$output_filename}\")\n 88 |     ),\n 89 |     upsert_at_end_of_each_plan = FALSE,\n 90 |     insert_at_end_of_each_plan = FALSE,\n 91 |     action_fn_name = \"scskeleton::weather_export_plots_action\",\n 92 |     data_selector_fn_name = \"scskeleton::weather_export_plots_data_selector\",\n 93 |     schema = list(\n 94 |       # input\n 95 |       \"anon_example_weather_data\" = sc::config$schemas$anon_example_weather_data\n 96 | \n 97 |       # output\n 98 |     ),\n 99 |     info = \"This task ploduces plots\"\n100 |   )\n101 | }\n\n05_deliverables.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/05_deliverables.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/05_deliverables.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 05_deliverables.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Set all the deliverables that team members are supposed to manually do/check\n 8 | #   every day/week/month.\n 9 | #\n10 | # ******************************************************************************\n11 | # ******************************************************************************\n12 | \n13 | set_deliverables <- function() {\n14 | \n15 | }\n\n06_config.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/06_config.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/06_config.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 06_config.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Call all the functions defined in 01, 02, 03, 04, and 05 in the correct order.\n 8 | #\n 9 | # PURPOSE 2:\n10 | #   Set all necessary configs that do not belong anywhere else.\n11 | #\n12 | #   E.g. Formatting for progress bars.\n13 | #\n14 | # ******************************************************************************\n15 | # ******************************************************************************\n16 | \n17 | set_config <- function() {\n18 |   # 01_definitions.r\n19 |   set_definitions()\n20 | \n21 |   # 02_permissions.r\n22 |   set_permissions()\n23 | \n24 |   # 03_db_schemas.r\n25 |   set_db_schemas()\n26 | \n27 |   # 04_tasks.r\n28 |   set_tasks()\n29 | \n30 |   # 05_deliverables.r\n31 |   set_deliverables()\n32 | \n33 |   # 06_config.r\n34 |   set_progressr()\n35 | }\n36 | \n37 | set_progressr <- function() {\n38 |   progressr::handlers(progressr::handler_progress(\n39 |     format = \"[:bar] :current/:total (:percent) in :elapsedfull, eta: :eta\",\n40 |     clear = FALSE\n41 |   ))\n42 | }\n\n07_onLoad.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/07_onLoad.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/07_onLoad.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 07_onLoad.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Initializing everything that happens when the package is loaded.\n 8 | #\n 9 | #   E.g. Calling bash scripts that authenticate against Kerebros, setting the\n10 | #   configs as defined in 06_config.r.\n11 | #\n12 | # ******************************************************************************\n13 | # ******************************************************************************\n14 | \n15 | .onLoad <- function(libname, pkgname) {\n16 |   # Mechanism to authenticate as necessary (e.g. Kerebros)\n17 |   try(system2(\"/bin/authenticate.sh\", stdout = NULL), TRUE)\n18 | \n19 |   # 5_config.r\n20 |   set_config()\n21 | \n22 |   # https://github.com/rstudio/rmarkdown/issues/1632\n23 |   assignInNamespace(\"clean_tmpfiles\", clean_tmpfiles_mod, ns = \"rmarkdown\")\n24 | \n25 |   invisible()\n26 | }\n\n08_onAttach.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/08_onAttach.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/08_onAttach.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 08_onAttach.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   What you want to happen when someone types library(yourpackage)\n 8 | #\n 9 | # ******************************************************************************\n10 | # ******************************************************************************\n11 | \n12 | .onAttach <- function(libname, pkgname) {\n13 | \n14 | }\n\n99_util_*.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/99_util_no_data_plot.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/99_util_no_data_plot.r\n\n 1 | # ******************************************************************************\n 2 | # ******************************************************************************\n 3 | #\n 4 | # 99_util_*.r\n 5 | #\n 6 | # PURPOSE 1:\n 7 | #   Utility functions that are used across multiple tasks\n 8 | #\n 9 | # ******************************************************************************\n10 | # ******************************************************************************\n11 | \n12 | no_data_plot <- function(){\n13 |   data=data.frame(x=0,y=0)\n14 |   q <- ggplot(data=data)\n15 |   q <- q + theme_void()\n16 |   q <- q + annotate(\"text\", label=glue::glue(\"Ikke noe data {fhi::nb$aa} vise\"), x=0, y=0, size=10)\n17 |   q\n18 | }\n\nTask files\nTask files are placed in .r files under their own names.\nweather_download_and_import_rawdata.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_download_and_import_rawdata.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_download_and_import_rawdata.r\n\n  1 | # **** action **** ----\n  2 | #' weather_download_and_import_rawdata (action)\n  3 | #' @param data Data\n  4 | #' @param argset Argset\n  5 | #' @param schema DB Schema\n  6 | #' @export\n  7 | weather_download_and_import_rawdata_action <- function(data, argset, schema) {\n  8 |   # tm_run_task(\"weather_download_and_import_rawdata\")\n  9 | \n 10 |   if (plnr::is_run_directly()) {\n 11 |     # sc::tm_get_plans_argsets_as_dt(\"weather_download_and_import_rawdata\")\n 12 | \n 13 |     index_plan <- 1\n 14 |     index_analysis <- 1\n 15 | \n 16 |     data <- sc::tm_get_data(\"weather_download_and_import_rawdata\", index_plan = index_plan)\n 17 |     argset <- sc::tm_get_argset(\"weather_download_and_import_rawdata\", index_plan = index_plan, index_analysis = index_analysis)\n 18 |     schema <- sc::tm_get_schema(\"weather_download_and_import_rawdata\")\n 19 |   }\n 20 | \n 21 |   # special case that runs before everything\n 22 |   if (argset$first_analysis == TRUE) {\n 23 | \n 24 |   }\n 25 | \n 26 |   a <- data$data\n 27 | \n 28 |   baz <- xml2::xml_find_all(a, \".//maxTemperature\")\n 29 |   res <- vector(\"list\", length = length(baz))\n 30 |   for (i in seq_along(baz)) {\n 31 |     parent <- xml2::xml_parent(baz[[i]])\n 32 |     grandparent <- xml2::xml_parent(parent)\n 33 |     time_from <- xml2::xml_attr(grandparent, \"from\")\n 34 |     time_to <- xml2::xml_attr(grandparent, \"to\")\n 35 |     x <- xml2::xml_find_all(parent, \".//minTemperature\")\n 36 |     temp_min <- xml2::xml_attr(x, \"value\")\n 37 |     x <- xml2::xml_find_all(parent, \".//maxTemperature\")\n 38 |     temp_max <- xml2::xml_attr(x, \"value\")\n 39 |     x <- xml2::xml_find_all(parent, \".//precipitation\")\n 40 |     precip <- xml2::xml_attr(x, \"value\")\n 41 |     res[[i]] <- data.frame(\n 42 |       time_from = as.character(time_from),\n 43 |       time_to = as.character(time_to),\n 44 |       temp_max = as.numeric(temp_max),\n 45 |       temp_min = as.numeric(temp_min),\n 46 |       precip = as.numeric(precip)\n 47 |     )\n 48 |   }\n 49 |   res <- rbindlist(res)\n 50 |   res <- res[stringr::str_sub(time_from, 12, 13) %in% c(\"00\", \"06\", \"12\", \"18\")]\n 51 |   res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]\n 52 |   res[, N := .N, by = date]\n 53 |   res <- res[N == 4]\n 54 |   res <- res[\n 55 |     ,\n 56 |     .(\n 57 |       temp_max = max(temp_max),\n 58 |       temp_min = min(temp_min),\n 59 |       precip = sum(precip)\n 60 |     ),\n 61 |     keyby = .(date)\n 62 |   ]\n 63 | \n 64 |   # we look at the downloaded data\n 65 |   # res\n 66 | \n 67 |   # we now need to format it\n 68 |   res[, granularity_time := \"day\"]\n 69 |   res[, sex := \"total\"]\n 70 |   res[, age := \"total\"]\n 71 |   res[, location_code := argset$location_code]\n 72 | \n 73 |   # fill in missing structural variables\n 74 |   sc::fill_in_missing_v8(res, border = 2020)\n 75 | \n 76 |   # we look at the downloaded data\n 77 |   # res\n 78 | \n 79 |   # put data in db table\n 80 |   sc::fill_in_missing_v8(res, border = config$border)\n 81 |   schema$anon_example_weather_rawdata$insert_data(res)\n 82 | \n 83 |   # special case that runs after everything\n 84 |   if (argset$last_analysis == TRUE) {\n 85 | \n 86 |   }\n 87 | }\n 88 | \n 89 | # **** data_selector **** ----\n 90 | #' weather_download_and_import_rawdata (data selector)\n 91 | #' @param argset Argset\n 92 | #' @param schema DB Schema\n 93 | #' @export\n 94 | weather_download_and_import_rawdata_data_selector <- function(argset, schema) {\n 95 |   if (plnr::is_run_directly()) {\n 96 |     # sc::tm_get_plans_argsets_as_dt(\"weather_download_and_import_rawdata\")\n 97 | \n 98 |     index_plan <- 1\n 99 | \n100 |     argset <- sc::tm_get_argset(\"weather_download_and_import_rawdata\", index_plan = index_plan)\n101 |     schema <- sc::tm_get_schema(\"weather_download_and_import_rawdata\")\n102 |   }\n103 | \n104 |   # find the mid lat/long for the specified location_code\n105 |   gps <- fhimaps::norway_lau2_map_b2020_default_dt[location_code == argset$location_code,.(\n106 |     lat = mean(lat),\n107 |     long = mean(long)\n108 |   )]\n109 | \n110 |   # download the forecast for the specified location_code\n111 |   d <- httr::GET(glue::glue(\"https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}\"), httr::content_type_xml())\n112 |   d <- xml2::read_xml(d$content)\n113 | \n114 |   # The variable returned must be a named list\n115 |   retval <- list(\n116 |     \"data\" = d\n117 |   )\n118 | \n119 |   retval\n120 | }\n121 | \n122 | # **** functions **** ----\n\nweather_clean_data.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_clean_data.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_clean_data.r\n\n  1 | # **** action **** ----\n  2 | #' weather_clean_data (action)\n  3 | #' @param data Data\n  4 | #' @param argset Argset\n  5 | #' @param schema DB Schema\n  6 | #' @export\n  7 | weather_clean_data_action <- function(data, argset, schema) {\n  8 |   # tm_run_task(\"weather_clean_data\")\n  9 | \n 10 |   if (plnr::is_run_directly()) {\n 11 |     # sc::tm_get_plans_argsets_as_dt(\"weather_clean_data\")\n 12 | \n 13 |     index_plan <- 1\n 14 |     index_analysis <- 1\n 15 | \n 16 |     data <- sc::tm_get_data(\"weather_clean_data\", index_plan = index_plan)\n 17 |     argset <- sc::tm_get_argset(\"weather_clean_data\", index_plan = index_plan, index_analysis = index_analysis)\n 18 |     schema <- sc::tm_get_schema(\"weather_clean_data\")\n 19 |   }\n 20 | \n 21 |   # special case that runs before everything\n 22 |   if (argset$first_analysis == TRUE) {\n 23 | \n 24 |   }\n 25 | \n 26 |   # make sure there's no missing data via the creation of a skeleton\n 27 |   # https://folkehelseinstituttet.github.io/fhidata/articles/Skeletons.html\n 28 | \n 29 |   # Create a variable (possibly a list) to hold the data\n 30 |   d_agg <- list()\n 31 |   d_agg$day_municip <- copy(data$day_municip)\n 32 | \n 33 |   # Pull out important dates\n 34 |   date_min <- min(d_agg$day_municip$date, na.rm = T)\n 35 |   date_max <- max(d_agg$day_municip$date, na.rm = T)\n 36 | \n 37 |   # Create `multiskeleton`\n 38 |   # granularity_geo should have the following groups:\n 39 |   # - nodata (when no data is available, and there is no \"finer\" data available to aggregate up)\n 40 |   # - all levels of granularity_geo where you have data available\n 41 |   # If you do not have data for a specific granularity_geo, but there is \"finer\" data available\n 42 |   # then you should not include this granularity_geo in the multiskeleton, because you will create\n 43 |   # it later when you aggregate up your data (baregion)\n 44 |   multiskeleton_day <- fhidata::make_skeleton(\n 45 |     date_min = date_min,\n 46 |     date_max = date_max,\n 47 |     granularity_geo = list(\n 48 |       \"nodata\" = c(\n 49 |         \"wardoslo\",\n 50 |         \"extrawardoslo\",\n 51 |         \"missingwardoslo\",\n 52 |         \"wardbergen\",\n 53 |         \"missingwardbergen\",\n 54 |         \"wardstavanger\",\n 55 |         \"missingwardstavanger\",\n 56 |         \"notmainlandmunicip\",\n 57 |         \"missingmunicip\",\n 58 |         \"notmainlandcounty\",\n 59 |         \"missingcounty\"\n 60 |       ),\n 61 |       \"municip\" = c(\n 62 |         \"municip\"\n 63 |       )\n 64 |     )\n 65 |   )\n 66 | \n 67 |   # Merge in the information you have at different geographical granularities\n 68 |   # one level at a time\n 69 |   # municip\n 70 |   multiskeleton_day$municip[\n 71 |     d_agg$day_municip,\n 72 |     on = c(\"location_code\", \"date\"),\n 73 |     c(\n 74 |       \"temp_max\",\n 75 |       \"temp_min\",\n 76 |       \"precip\"\n 77 |     ) := .(\n 78 |       temp_max,\n 79 |       temp_min,\n 80 |       precip\n 81 |     )\n 82 |   ]\n 83 | \n 84 |   multiskeleton_day$municip[]\n 85 | \n 86 |   # Aggregate up to higher geographical granularities (county)\n 87 |   multiskeleton_day$county <- multiskeleton_day$municip[\n 88 |     fhidata::norway_locations_hierarchy(\n 89 |       from = \"municip\",\n 90 |       to = \"county\"\n 91 |     ),\n 92 |     on = c(\n 93 |       \"location_code==from_code\"\n 94 |     )\n 95 |   ][,\n 96 |     .(\n 97 |       temp_max = mean(temp_max, na.rm = T),\n 98 |       temp_min = mean(temp_min, na.rm = T),\n 99 |       precip = mean(precip, na.rm = T),\n100 |       granularity_geo = \"county\"\n101 |     ),\n102 |     by = .(\n103 |       granularity_time,\n104 |       date,\n105 |       location_code = to_code\n106 |     )\n107 |   ]\n108 | \n109 |   multiskeleton_day$county[]\n110 | \n111 |   # Aggregate up to higher geographical granularities (nation)\n112 |   multiskeleton_day$nation <- multiskeleton_day$municip[\n113 |     ,\n114 |     .(\n115 |       temp_max = mean(temp_max, na.rm = T),\n116 |       temp_min = mean(temp_min, na.rm = T),\n117 |       precip = mean(precip, na.rm = T),\n118 |       granularity_geo = \"nation\",\n119 |       location_code = \"norge\"\n120 |     ),\n121 |     by = .(\n122 |       granularity_time,\n123 |       date\n124 |     )\n125 |   ]\n126 | \n127 |   multiskeleton_day$nation[]\n128 | \n129 |   # combine all the different granularity_geos\n130 |   skeleton_day <- rbindlist(multiskeleton_day, fill = TRUE, use.names = TRUE)\n131 | \n132 |   skeleton_day[]\n133 | \n134 |   # 10. (If desirable) aggregate up to higher time granularities\n135 |   # if necessary, it is now easy to aggregate up to weekly data from here\n136 |   skeleton_isoweek <- copy(skeleton_day)\n137 |   skeleton_isoweek[, isoyearweek := fhiplot::isoyearweek_c(date)]\n138 |   skeleton_isoweek <- skeleton_isoweek[\n139 |     ,\n140 |     .(\n141 |       temp_max = mean(temp_max, na.rm = T),\n142 |       temp_min = mean(temp_min, na.rm = T),\n143 |       precip = mean(precip, na.rm = T),\n144 |       granularity_time = \"isoweek\"\n145 |     ),\n146 |     keyby = .(\n147 |       isoyearweek,\n148 |       granularity_geo,\n149 |       location_code\n150 |     )\n151 |   ]\n152 | \n153 |   skeleton_isoweek[]\n154 | \n155 |   # we now need to format it and fill in missing structural variables\n156 |   # day\n157 |   skeleton_day[, sex := \"total\"]\n158 |   skeleton_day[, age := \"total\"]\n159 |   sc::fill_in_missing_v8(skeleton_day, border = config$border)\n160 | \n161 |   # isoweek\n162 |   skeleton_isoweek[, sex := \"total\"]\n163 |   skeleton_isoweek[, age := \"total\"]\n164 |   sc::fill_in_missing_v8(skeleton_isoweek, border = config$border)\n165 |   skeleton_isoweek[, date := as.Date(date)]\n166 | \n167 |   skeleton <- rbindlist(\n168 |     list(\n169 |       skeleton_day,\n170 |       skeleton_isoweek\n171 |     ),\n172 |     use.names = T\n173 |   )\n174 | \n175 |   # put data in db table\n176 |   schema$anon_example_weather_data$drop_all_rows_and_then_insert_data(skeleton)\n177 | \n178 |   # special case that runs after everything\n179 |   if (argset$last_analysis == TRUE) {\n180 | \n181 |   }\n182 | }\n183 | \n184 | # **** data_selector **** ----\n185 | #' weather_clean_data (data selector)\n186 | #' @param argset Argset\n187 | #' @param schema DB Schema\n188 | #' @export\n189 | weather_clean_data_data_selector <- function(argset, schema) {\n190 |   if (plnr::is_run_directly()) {\n191 |     # sc::tm_get_plans_argsets_as_dt(\"weather_clean_data\")\n192 | \n193 |     index_plan <- 1\n194 | \n195 |     argset <- sc::tm_get_argset(\"weather_clean_data\", index_plan = index_plan)\n196 |     schema <- sc::tm_get_schema(\"weather_clean_data\")\n197 |   }\n198 | \n199 |   # The database schemas can be accessed here\n200 |   d <- schema$anon_example_weather_rawdata$tbl() %>%\n201 |     sc::mandatory_db_filter(\n202 |       granularity_time = \"day\",\n203 |       granularity_time_not = NULL,\n204 |       granularity_geo = \"municip\",\n205 |       granularity_geo_not = NULL,\n206 |       country_iso3 = NULL,\n207 |       location_code = NULL,\n208 |       age = \"total\",\n209 |       age_not = NULL,\n210 |       sex = \"total\",\n211 |       sex_not = NULL\n212 |     ) %>%\n213 |     dplyr::select(\n214 |       granularity_time,\n215 |       # granularity_geo,\n216 |       # country_iso3,\n217 |       location_code,\n218 |       # border,\n219 |       # age,\n220 |       # sex,\n221 | \n222 |       date,\n223 | \n224 |       # isoyear,\n225 |       # isoweek,\n226 |       # isoyearweek,\n227 |       # season,\n228 |       # seasonweek,\n229 | \n230 |       # calyear,\n231 |       # calmonth,\n232 |       # calyearmonth,\n233 | \n234 |       temp_max,\n235 |       temp_min,\n236 |       precip\n237 |     ) %>%\n238 |     dplyr::collect() %>%\n239 |     as.data.table() %>%\n240 |     setorder(\n241 |       location_code,\n242 |       date\n243 |     )\n244 | \n245 |   # The variable returned must be a named list\n246 |   retval <- list(\n247 |     \"day_municip\" = d\n248 |   )\n249 | \n250 |   retval\n251 | }\n252 | \n253 | # **** functions **** ----\n\nweather_export_weather_plots.r\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_export_plots.r\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/weather_export_plots.r\n\n  1 | # **** action **** ----\n  2 | #' weather_export_plots (action)\n  3 | #' @param data Data\n  4 | #' @param argset Argset\n  5 | #' @param schema DB Schema\n  6 | #' @export\n  7 | weather_export_plots_action <- function(data, argset, schema) {\n  8 |   # tm_run_task(\"weather_export_plots\")\n  9 | \n 10 |   if(plnr::is_run_directly()){\n 11 |     # sc::tm_get_plans_argsets_as_dt(\"weather_export_plots\")\n 12 | \n 13 |     index_plan <- 1\n 14 |     index_analysis <- 1\n 15 | \n 16 |     data <- sc::tm_get_data(\"weather_export_plots\", index_plan = index_plan)\n 17 |     argset <- sc::tm_get_argset(\"weather_export_plots\", index_plan = index_plan, index_analysis = index_analysis)\n 18 |     schema <- sc::tm_get_schema(\"weather_export_plots\")\n 19 |   }\n 20 | \n 21 |   # code goes here\n 22 |   # special case that runs before everything\n 23 |   if(argset$first_analysis == TRUE){\n 24 | \n 25 |   }\n 26 | \n 27 |   # create the output_dir (if it doesn't exist)\n 28 |   fs::dir_create(glue::glue(argset$output_dir))\n 29 | \n 30 |   q <- ggplot(data$data, aes(x = date, ymin = temp_min, ymax = temp_max))\n 31 |   q <- q + geom_ribbon(alpha = 0.5)\n 32 | \n 33 |   ggsave(\n 34 |     filename = glue::glue(argset$output_absolute_path),\n 35 |     plot = q\n 36 |   )\n 37 | \n 38 |   # special case that runs after everything\n 39 |   # copy to anon_web?\n 40 |   if(argset$last_analysis == TRUE){\n 41 | \n 42 |   }\n 43 | }\n 44 | \n 45 | # **** data_selector **** ----\n 46 | #' weather_export_plots (data selector)\n 47 | #' @param argset Argset\n 48 | #' @param schema DB Schema\n 49 | #' @export\n 50 | weather_export_plots_data_selector = function(argset, schema){\n 51 |   if(plnr::is_run_directly()){\n 52 |     # sc::tm_get_plans_argsets_as_dt(\"weather_export_plots\")\n 53 | \n 54 |     index_plan <- 1\n 55 | \n 56 |     argset <- sc::tm_get_argset(\"weather_export_plots\", index_plan = index_plan)\n 57 |     schema <- sc::tm_get_schema(\"weather_export_plots\")\n 58 |   }\n 59 | \n 60 |   # The database schemas can be accessed here\n 61 |   d <- schema$anon_example_weather_data$tbl() %>%\n 62 |     sc::mandatory_db_filter(\n 63 |       granularity_time = NULL,\n 64 |       granularity_time_not = NULL,\n 65 |       granularity_geo = NULL,\n 66 |       granularity_geo_not = NULL,\n 67 |       country_iso3 = NULL,\n 68 |       location_code = argset$location_code,\n 69 |       age = NULL,\n 70 |       age_not = NULL,\n 71 |       sex = NULL,\n 72 |       sex_not = NULL\n 73 |     ) %>%\n 74 |     dplyr::select(\n 75 |       # granularity_time,\n 76 |       # granularity_geo,\n 77 |       # country_iso3,\n 78 |       # location_code,\n 79 |       # border,\n 80 |       # age,\n 81 |       # sex,\n 82 | \n 83 |       date,\n 84 | \n 85 |       # isoyear,\n 86 |       # isoweek,\n 87 |       # isoyearweek,\n 88 |       # season,\n 89 |       # seasonweek,\n 90 |       #\n 91 |       # calyear,\n 92 |       # calmonth,\n 93 |       # calyearmonth,\n 94 | \n 95 |       temp_max,\n 96 |       temp_min\n 97 |     ) %>%\n 98 |     dplyr::collect() %>%\n 99 |     as.data.table() %>%\n100 |     setorder(\n101 |       # location_code,\n102 |       date\n103 |     )\n104 | \n105 |   # The variable returned must be a named list\n106 |   retval <- list(\n107 |     \"data\" = d\n108 |   )\n109 |   retval\n110 | }\n111 | \n112 | # **** functions **** ----\n113 | \n114 | \n115 | \n116 | \n\nChangelog\n2021-07-14: Draft created.\n\n\n\n",
      "last_modified": "2021-07-15T11:13:12+02:00"
    },
    {
      "path": "concepts_tasks.html",
      "title": "Tasks",
      "description": "What are tasks, and how do they work in Sykdomspulsen Core?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-05-26",
      "contents": "\n\nContents\nIntroduction\nDefinitions\nGeneral tasks\nPutting it together\nWeather example\naction_fn\nRun the task\nExamples of different types of tasks\nChangelog\n\nIntroduction\nA task is the basic operational unit of Sykdomspulsen Core. It is based on plnr.\nIn short, you can think of a Sykdomspulsen Core task as multiple plnr plans plus Sykdomspulsen Core db schemas.\nDefinitions\n\n\nObject\n\n\nDescription\n\n\nargset\n\n\nA named list containing arguments.\n\n\nplnr analysis\n\n\nThese are the fundamental units that are scheduled in plnr:\n\n1 argset\n\n\n1 function that takes two (or more) arguments:\n\ndata (named list)\n\n\nargset (named list)\n\n\n… (optional arguments)\n\n\n\n\ndata_selector_fn\n\n\nA function that takes two arguments:\n\nargset (named list)\n\n\nschema (named list)\n\n\nThis function provides a named list to be used as the data argument to action_fn\n\n\naction_fn\n\n\nA function that takes three arguments:\n\ndata (named list, returned from data_selector_fn)\n\n\nargset (named list)\n\n\nschema (named list)\n\n\nThis is the thing that ‘does stuff’ in Sykdomspulsen Core.\n\n\nsc analysis\n\n\nA sc analysis is essentially a plnr analysis with database schemas:\n\n1 argset\n\n\n1 action_fn\n\n\n\nplan\n\n\n\n1 data-pull (using data_selector_fn)\n\n\n1 list of sc analyses\n\n\n\ntask\n\n\nThis is is the unit that Airflow schedules.\n\n1 list of plans\n\n\nWe sometimes run the list of plans in parallel.\n\n\nGeneral tasks\n\n\n\nFigure 1: A general task showing the many options of a task.\n\n\n\nFigure 1 shows us the full potential of a task.\nData can be read from any sources, then within a plan the data will be extracted once by data_selector_fn (i.e. “one data-pull”). The data will then be provided to each analysis, which will run action_fn on:\nThe provided data\nThe provided argset\nThe provided schemas\nThe action_fn can then:\nWrite data/results to db schemas\nSend emails\nExport graphs, excel files, reports, or other physical files\nTypically only a subset of this would be done in a single task.\nPlan-heavy or analysis-heavy tasks?\nA plan-heavy task is one that has many plans and a few analyses per plan.\nAn analysis-heavy task is one that has few plans and many analyses per plan.\nIn general, a data-pull is slow and wastes time. This means that it is preferable to reduce the number of data-pulls performed by having each data-pull extract larger quantities of data. The analysis can then subset the data as required (identifed via argsets). i.e. If possible, an analysis-heavy task is preferable because it will be faster (at the cost of needing more RAM).\nObviously, if a plan’s data-pull is larger, it will use more RAM. If you need to conserve RAM, then you should use a plan-heavy approach.\nFigure 1 shows only 2 location based analyses, but in reality there are 356 municipalities in Norway in 2021. If figure 1 had 2 plans (1 for 2021 data, 1 for 2020 data) and 356 analyses for each plan (1 for each location_code) then we would be taking an analysis-heavy approach.\nPutting it together\n\n\n\nFigure 2: A typical file setup for an implementation of Sykdomspulsen Core. plan_argset_fn is rarely used, and is therefore shown as blacked out in the most of the tasks.\n\n\n\nFigure 2 shows a typical implementation of Sykdomspulsen Core.\nconfig_db.r contains all of the Sykdomspulsen Core db schemas definitions. i.e. A long list of sc::add_schema_v8 commands.\nconfig_tasks.r contains all of the task definitions. i.e. A long list of sc::add_task_from_config_v8 commands.\nThen we have a one file for each task that contains the action_fn, data_selector_fn and other functions that are relevant to the task at hand.\nWeather example\nWe will now go through an example of how a person would design and implement tasks relating to weather\ndb schema\nAs documented in more detail here, we create a db schema that fits our needs (recording weather data).\n\n\n\n\n\nsc::add_schema_v8(\n  name_access = c(\"anon\"),\n  name_grouping = \"example_weather\",\n  name_variant = NULL,\n  db_configs = sc::config$db_configs,\n  field_types =  c(\n    \"granularity_time\" = \"TEXT\",\n    \"granularity_geo\" = \"TEXT\",\n    \"country_iso3\" = \"TEXT\",\n    \"location_code\" = \"TEXT\",\n    \"border\" = \"INTEGER\",\n    \"age\" = \"TEXT\",\n    \"sex\" = \"TEXT\",\n    \n    \"date\" = \"DATE\",\n    \n    \"isoyear\" = \"INTEGER\",\n    \"isoweek\" = \"INTEGER\",\n    \"isoyearweek\" = \"TEXT\",\n    \"season\" = \"TEXT\",\n    \"seasonweek\" = \"DOUBLE\",\n    \n    \"calyear\" = \"INTEGER\",\n    \"calmonth\" = \"INTEGER\",\n    \"calyearmonth\" = \"TEXT\",\n\n    \"tg\" = \"DOUBLE\",\n    \"tx\" = \"DOUBLE\",\n    \"tn\" = \"DOUBLE\",\n    \"rr\" = \"DOUBLE\"\n  ),\n  keys = c(\n    \"granularity_time\",\n    \"location_code\",\n    \"date\",\n    \"age\",\n    \"sex\"\n  ),\n  censors = list(\n    anon = list(\n      \n    )\n  ),\n  validator_field_types = sc::validator_field_types_sykdomspulsen,\n  validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n  info = \"This db table is used for...\"\n)\n\n\n\ntask_from_config_v8\nTo “register” our task, we use the RStudio addin task_from_config.\n\n\n\n\n\n# tm_run_task(\"example_weather_import_data_from_api\")\nsc::add_task_from_config_v8(\n  name_grouping = \"example_weather\",\n  name_action = \"import_data_from_api\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, # \"PACKAGE::TASK_NAME_plan_analysis\"\n  for_each_plan = plnr::expand_list(\n    location_code = \"county03\" # fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = NULL,\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_weather_import_data_from_api_action\",\n  data_selector_fn_name = \"example_weather_import_data_from_api_data_selector\",\n  schema = list(\n    # input\n\n    # output\n    \"anon_example_weather\" = sc::config$schemas$anon_example_weather\n  ),\n  info = \"This task does...\"\n)\n\n\n\nThere are a number of important things in this code that need highlighting.\nfor_each_plan\nfor_each_plan expects a list. Each component of the list will correspond to a plan, with the values added to the argset of all the analyses inside the plan.\nFor example, the following code would give 4 plans, with 1 analysis per each plan, with each analysis containing argset$var_1 and argset$var_2 as appropriate.\n\n\nfor_each_plan <- list()\nfor_each_plan[[1]] <- list(\n  var_1 = 1,\n  var_2 = \"a\"\n)\nfor_each_plan[[2]] <- list(\n  var_1 = 2,\n  var_2 = \"b\"\n)\nfor_each_plan[[3]] <- list(\n  var_1 = 1,\n  var_2 = \"a\"\n)\nfor_each_plan[[4]] <- list(\n  var_1 = 2,\n  var_2 = \"b\"\n)\n\n\n\nYou always need at least 1 plan. The most simple plan possible is:\n\n\nplnr::expand_list(\n  x = 1\n)\n\n\n[[1]]\n[[1]]$x\n[1] 1\n\nplnr::expand_list\nplnr::expand_list is esentially the same as expand.grid, except that its return values are lists instead of data.frame.\nThe code above could be simplified as follows.\n\n\nfor_each_plan <- plnr::expand_list(\n  var_1 = c(1,2),\n  var_2 = c(\"a\", \"b\")\n)\nfor_each_plan\n\n\n[[1]]\n[[1]]$var_1\n[1] 1\n\n[[1]]$var_2\n[1] \"a\"\n\n\n[[2]]\n[[2]]$var_1\n[1] 2\n\n[[2]]$var_2\n[1] \"a\"\n\n\n[[3]]\n[[3]]$var_1\n[1] 1\n\n[[3]]$var_2\n[1] \"b\"\n\n\n[[4]]\n[[4]]$var_1\n[1] 2\n\n[[4]]$var_2\n[1] \"b\"\n\nfor_each_analysis\nfor_each_plan expects a list, which will generate length(for_each_plan) plans.\nfor_each_analysis is the same, except it will generate analyses within each of the plans.\nuniversal_argset\nA named list that will add the values to the argset of all the analyses.\nupsert_at_end_of_each_plan\nIf TRUE and schema contains a schema called output, then the returned values of action_fn will be stored and upserted to schema$output at the end of each plan.\nIf you choose to upsert/insert manually from within action_fn, you can only do so at the end of each analysis.\ninsert_at_end_of_each_plan\nIf TRUE and schema contains a schema called output, then the returned values of action_fn will be stored and inserted to schema$output at the end of each plan.\nIf you choose to upsert/insert manually from within action_fn, you can only do so at the end of each analysis.\naction_fn_name\nA character string of the action_fn, preferably including the package name.\ndata_selector_fn_name\nA character string of the data_selector_fn, preferably including the package name.\nschema\nA named list containing the schemas used in this task.\ndata_selector_fn\nUse the addins dropdown to easily add in boilerplate code.\n\n\n\nThe data_selector_fn is used to extract the data for each plan.\nThe lines inside if(plnr::is_run_directly()){ are used to help developers. You can run the code manually/interactively to “load” the values of argset and schema.\n\n\nindex_plan <- 1\n\nargset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan)\nschema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n\nprint(argset)\n\n\n$`**universal**`\n[1] \"*\"\n\n$`**plan**`\n[1] \"*\"\n\n$location_code\n[1] \"county03\"\n\n$`**analysis**`\n[1] \"*\"\n\n$`**automatic**`\n[1] \"*\"\n\n$index\n[1] 1\n\n$today\n[1] \"2021-07-15\"\n\n$yesterday\n[1] \"2021-07-14\"\n\n$first_analysis\n[1] TRUE\n\n$first_argset\n[1] TRUE\n\n$last_analysis\n[1] TRUE\n\n$last_argset\n[1] TRUE\n\nprint(names(schema))\n\n\n[1] \"anon_example_weather\"\n\n\n\n# **** data_selector **** ----\n#' example_weather_import_data_from_api (data selector)\n#' @param argset Argset\n#' @param schema DB Schema\n#' @export\nexample_weather_import_data_from_api_data_selector = function(argset, schema){\n  if(plnr::is_run_directly()){\n    # sc::tm_get_plans_argsets_as_dt(\"example_weather_import_data_from_api\")\n\n    index_plan <- 1\n\n    argset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan)\n    schema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n  }\n\n  # find the mid lat/long for the specified location_code\n  gps <- fhimaps::norway_nuts3_map_b2020_default_dt[location_code == argset$location_code,.(\n    lat = mean(lat),\n    long = mean(long)\n  )]\n  \n  # download the forecast for the specified location_code\n  d <- httr::GET(glue::glue(\"https://api.met.no/weatherapi/locationforecast/2.0/classic?lat={gps$lat}&lon={gps$long}\"), httr::content_type_xml())\n  d <- xml2::read_xml(d$content)\n\n  # The variable returned must be a named list\n  retval <- list(\n    \"data\" = d\n  )\n  retval\n}\n\n\n\naction_fn\nThe lines inside if(plnr::is_run_directly()){ are used to help developers. You can run the code manually/interactively to “load” the values of argset and schema.\n\n\nindex_plan <- 1\nindex_analysis <- 1\n\ndata <- sc::tm_get_data(\"example_weather_import_data_from_api\", index_plan = index_plan)\nargset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan, index_analysis = index_analysis)\nschema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n\nprint(data)\n\n\n$data\n{xml_document}\n<weatherdata noNamespaceSchemaLocation=\"https://schema.api.met.no/schemas/weatherapi-0.4.xsd\" created=\"2021-07-15T08:58:11Z\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n[1] <meta>\\n  <model name=\"met_public_forecast\" termin=\"2021-07-15T ...\n[2] <product class=\"pointData\">\\n  <time datatype=\"forecast\" from=\" ...\n\nprint(argset)\n\n\n$`**universal**`\n[1] \"*\"\n\n$`**plan**`\n[1] \"*\"\n\n$location_code\n[1] \"county03\"\n\n$`**analysis**`\n[1] \"*\"\n\n$`**automatic**`\n[1] \"*\"\n\n$index\n[1] 1\n\n$today\n[1] \"2021-07-15\"\n\n$yesterday\n[1] \"2021-07-14\"\n\n$first_analysis\n[1] TRUE\n\n$first_argset\n[1] TRUE\n\n$last_analysis\n[1] TRUE\n\n$last_argset\n[1] TRUE\n\nprint(names(schema))\n\n\n[1] \"anon_example_weather\"\n\n\n\n# **** action **** ----\n#' example_weather_import_data_from_api (action)\n#' @param data Data\n#' @param argset Argset\n#' @param schema DB Schema\n#' @export\nexample_weather_import_data_from_api_action <- function(data, argset, schema) {\n  # tm_run_task(\"example_weather_import_data_from_api\")\n\n  if(plnr::is_run_directly()){\n    # sc::tm_get_plans_argsets_as_dt(\"example_weather_import_data_from_api\")\n\n    index_plan <- 1\n    index_analysis <- 1\n\n    data <- sc::tm_get_data(\"example_weather_import_data_from_api\", index_plan = index_plan)\n    argset <- sc::tm_get_argset(\"example_weather_import_data_from_api\", index_plan = index_plan, index_analysis = index_analysis)\n    schema <- sc::tm_get_schema(\"example_weather_import_data_from_api\")\n  }\n\n  # code goes here\n  # special case that runs before everything\n  if(argset$first_analysis == TRUE){\n\n  }\n  \n  a <- data$data\n  \n  baz <- xml2::xml_find_all(a, \".//maxTemperature\")\n  res <- vector(\"list\", length = length(baz))\n  for (i in seq_along(baz)) {\n    parent <- xml2::xml_parent(baz[[i]])\n    grandparent <- xml2::xml_parent(parent)\n    time_from <- xml2::xml_attr(grandparent, \"from\")\n    time_to <- xml2::xml_attr(grandparent, \"to\")\n    x <- xml2::xml_find_all(parent, \".//minTemperature\")\n    temp_min <- xml2::xml_attr(x, \"value\")\n    x <- xml2::xml_find_all(parent, \".//maxTemperature\")\n    temp_max <- xml2::xml_attr(x, \"value\")\n    x <- xml2::xml_find_all(parent, \".//precipitation\")\n    precip <- xml2::xml_attr(x, \"value\")\n    res[[i]] <- data.frame(\n      time_from = as.character(time_from),\n      time_to = as.character(time_to),\n      tx = as.numeric(temp_max),\n      tn = as.numeric(temp_min),\n      rr = as.numeric(precip)\n    )\n  }\n  res <- rbindlist(res)\n  res <- res[stringr::str_sub(time_from, 12, 13) %in% c(\"00\", \"06\", \"12\", \"18\")]\n  res[, date := as.Date(stringr::str_sub(time_from, 1, 10))]\n  res[, N := .N, by = date]\n  res <- res[N == 4]\n  res <- res[\n    , \n    .(\n      tg = NA,\n      tx = max(tx),\n      tn = min(tn),\n      rr = sum(rr)\n    ),\n    keyby = .(date)\n  ]\n  \n  # we look at the downloaded data\n  print(\"Data after downloading\")\n  print(res)\n  \n  # we now need to format it\n  res[, granularity_time := \"day\"]\n  res[, sex := \"total\"]\n  res[, age := \"total\"]\n  res[, location_code := argset$location_code]\n  \n  # fill in missing structural variables\n  sc::fill_in_missing_v8(res, border = 2020)\n  \n  # we look at the downloaded data\n  print(\"Data after missing structural variables filled in\")\n  print(res)\n\n  # put data in db table\n  # schema$SCHEMA_NAME$insert_data(d)\n  schema$anon_example_weather$upsert_data(res)\n  # schema$SCHEMA_NAME$drop_all_rows_and_then_upsert_data(d)\n\n  # special case that runs after everything\n  # copy to anon_web?\n  if(argset$last_analysis == TRUE){\n    # sc::copy_into_new_table_where(\n    #   table_from = \"anon_X\",\n    #   table_to = \"anon_webkht\"\n    # )\n  }\n}\n\n\n\nRun the task\n\n\ntm_run_task(\"example_weather_import_data_from_api\")\n\n\n[1] \"Data after downloading\"\n         date tg   tx   tn rr\n1: 2021-07-16 NA 27.1 17.8  0\n2: 2021-07-17 NA 27.4 16.6  0\n3: 2021-07-18 NA 20.9 13.5  0\n4: 2021-07-19 NA 22.5 10.4  0\n5: 2021-07-20 NA 23.6 11.6  0\n6: 2021-07-21 NA 23.8 11.3  0\n7: 2021-07-22 NA 25.3 12.0  0\n8: 2021-07-23 NA 24.5 12.7  0\n[1] \"Data after missing structural variables filled in\"\n         date tg   tx   tn rr granularity_time   sex   age\n1: 2021-07-16 NA 27.1 17.8  0              day total total\n2: 2021-07-17 NA 27.4 16.6  0              day total total\n3: 2021-07-18 NA 20.9 13.5  0              day total total\n4: 2021-07-19 NA 22.5 10.4  0              day total total\n5: 2021-07-20 NA 23.6 11.6  0              day total total\n6: 2021-07-21 NA 23.8 11.3  0              day total total\n7: 2021-07-22 NA 25.3 12.0  0              day total total\n8: 2021-07-23 NA 24.5 12.7  0              day total total\n   location_code granularity_geo border isoyearweek    season isoyear\n1:      county03          county   2020     2021-28 2020/2021    2021\n2:      county03          county   2020     2021-28 2020/2021    2021\n3:      county03          county   2020     2021-28 2020/2021    2021\n4:      county03          county   2020     2021-29 2020/2021    2021\n5:      county03          county   2020     2021-29 2020/2021    2021\n6:      county03          county   2020     2021-29 2020/2021    2021\n7:      county03          county   2020     2021-29 2020/2021    2021\n8:      county03          county   2020     2021-29 2020/2021    2021\n   isoweek seasonweek calyear calmonth calyearmonth country_iso3\n1:      28         51    2021        7     2021-M07          nor\n2:      28         51    2021        7     2021-M07          nor\n3:      28         51    2021        7     2021-M07          nor\n4:      29         52    2021        7     2021-M07          nor\n5:      29         52    2021        7     2021-M07          nor\n6:      29         52    2021        7     2021-M07          nor\n7:      29         52    2021        7     2021-M07          nor\n8:      29         52    2021        7     2021-M07          nor\n\nExamples of different types of tasks\nImporting data\n\n\nknitr::include_graphics(\"analytics_tasks_introduction/task_import_data.png\")\n\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"import_data\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL,\n  for_each_plan = plnr::expand_list(\n    x = 1\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"input\", \"example\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_import_data_action\",\n  data_selector_fn_name = \"example_import_data_data_selector\",\n  schema = list(\n    # input\n\n    # output\n    \"output\" = sc::config$schemas$output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nAnalysis\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"analysis\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = NULL,\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_analysis_action\",\n  data_selector_fn_name = \"example_analysis_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input,\n\n    # output\n    \"output\" = sc::config$schemas$output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nExporting multiple sets of results\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"export_results\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    location_code = fhidata::norway_locations_names()[granularity_geo %in% c(\"county\")]$location_code\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"output\", \"example\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_export_results_action\",\n  data_selector_fn_name = \"example_export_results_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input\n\n    # output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nExporting combined results\n\n\n\n\n\nsc::add_task_from_config_v8(\n  name_grouping = \"example\",\n  name_action = \"export_results\",\n  name_variant = NULL,\n  cores = 1,\n  plan_analysis_fn_name = NULL, \n  for_each_plan = plnr::expand_list(\n    x = 1\n  ),\n  for_each_analysis = NULL,\n  universal_argset = list(\n    folder = sc::path(\"output\", \"example\"),\n    granularity_geos = c(\"nation\", \"county\")\n  ),\n  upsert_at_end_of_each_plan = FALSE,\n  insert_at_end_of_each_plan = FALSE,\n  action_fn_name = \"example_export_results_action\",\n  data_selector_fn_name = \"example_export_results_data_selector\",\n  schema = list(\n    # input\n    \"input\" = sc::config$schemas$input\n\n    # output\n  ),\n  info = \"This task does...\"\n)\n\n\n\nChangelog\n2021-05-26: Draft created.\n\n\n\n",
      "last_modified": "2021-07-15T11:13:17+02:00"
    },
    {
      "path": "index.html",
      "title": "Sykdomspulsen Core",
      "description": "A free and open-source health surveillance system\n",
      "author": [],
      "contents": "\nSykdomspulsen core is a free and open-source health surveillance system designed and developed by the Norwegian Institute of Public Health.\nThe package is undergoing development now at https://github.com/folkehelseinstituttet/sc\nYou can subscribe to our newsletter to be notified when Sykdomspulsen Core is ready for public use.\n\n\n\n\n\n",
      "last_modified": "2021-07-15T11:13:17+02:00"
    },
    {
      "path": "tutorial_1_introduction.html",
      "title": "Tutorial 1: Introduction",
      "description": "Which files go where, and what is in them?\n",
      "author": [
        {
          "name": {},
          "url": "https://www.fhi.no"
        }
      ],
      "date": "2021-07-15",
      "contents": "\n\nContents\nSetup\nLoad the code\nRunning\nDevelop\nChangelog\n\nSetup\nImplementing Sykdomspulsen Core requires a number of functions to be called in the correct order. To make this as simple as possible, we have provided a skeleton implementation at https://github.com/folkehelseinstituttet/scskeleton\nYou should clone this GitHub repo (https://github.com/folkehelseinstituttet/scskeleton) to your server. This will be the package that you will be working on throughout this tutorial. You may choose to do a global find/replace on scskeleton with the name you want for your R package. We will refer to this R package as your “sc implementation”.\nYou should also clone https://github.com/folkehelseinstituttet/scexample to your server. This is the end product of the tutorial, and you should refer to it in order to check your work.\nFor the purposes of this tutorial, we assume that the reader is either using RStudio Server Open Source or RStudio Workbench inside Docker containers that have been built according to the Sykdomspulsen specifications. We will refer to your implementation of RStudio Server Open Source/RStudio Workbench with the generic term “RStudio”.\nLoad the code\n\nIn general, we recommend cleaning your working environment every time before running devtools::load_all(\".\").\nOpen scskeleton in RStudio project mode. Restart the R session via Ctrl+Shift+F10, rstudioapi::restartSession(), or Session > Restart R. This will ensure that you have a clean working environment before you begin. You may now load your sc implementation. This can be done via Ctrl+Shift+L, devtools::load_all(\".\"), or Build > Load All.\n\n\nrstudioapi::restartSession()\ndevtools::load_all(\".\")\n\n\n\nYou can now see which schemas have been loaded. These schemas were included in the skeleton.\n\n\nsc::tm_get_schema_names()\n\n\n [1] \"config_last_updated\"          \"config_structure_time\"       \n [3] \"rundate\"                      \"config_datetime\"             \n [5] \"anon_example_weather_rawdata\" \"anon_example_weather_data\"   \n [7] \"restr_example\"                \"anon_example\"                \n [9] \"redirect_example\"             \"anon_example_weather\"        \n\nYou can now see which tasks have been loaded. These tasks were included in the skeleton.\n\n\nsc::tm_get_task_names()\n\n\n[1] \"weather_download_and_import_rawdata\" \n[2] \"weather_clean_data\"                  \n[3] \"weather_export_plots\"                \n[4] \"example_weather_import_data_from_api\"\n\nRunning\nYou can now run these tasks. Note that we use scskeleton::tm_run_task instead of sc::tm_run_task. This is because we want to ensure that scexample::.onLoad has been called.\n\n\nscskeleton::tm_run_task(\"weather_download_and_import_rawdata\")\nscskeleton::tm_run_task(\"weather_clean_data\")\nscskeleton::tm_run_task(\"weather_export_weather_plots\")\n\n\n\nDevelop\nThe first step when developing any task is specifying the schemas that will be used.\n\nhttps://github.com/folkehelseinstituttet/scskeleton/blob/main/R/03_db_schemas.r#L19-L64\n\n19 |   sc::add_schema_v8(\n20 |     name_access = c(\"anon\"),\n21 |     name_grouping = \"example_weather\",\n22 |     name_variant = \"rawdata\",\n23 |     db_configs = sc::config$db_configs,\n24 |     field_types =  c(\n25 |       \"granularity_time\" = \"TEXT\",\n26 |       \"granularity_geo\" = \"TEXT\",\n27 |       \"country_iso3\" = \"TEXT\",\n28 |       \"location_code\" = \"TEXT\",\n29 |       \"border\" = \"INTEGER\",\n30 |       \"age\" = \"TEXT\",\n31 |       \"sex\" = \"TEXT\",\n32 | \n33 |       \"date\" = \"DATE\",\n34 | \n35 |       \"isoyear\" = \"INTEGER\",\n36 |       \"isoweek\" = \"INTEGER\",\n37 |       \"isoyearweek\" = \"TEXT\",\n38 |       \"season\" = \"TEXT\",\n39 |       \"seasonweek\" = \"DOUBLE\",\n40 | \n41 |       \"calyear\" = \"INTEGER\",\n42 |       \"calmonth\" = \"INTEGER\",\n43 |       \"calyearmonth\" = \"TEXT\",\n44 | \n45 |       \"temp_max\" = \"DOUBLE\",\n46 |       \"temp_min\" = \"DOUBLE\",\n47 |       \"precip\" = \"DOUBLE\"\n48 |     ),\n49 |     keys = c(\n50 |       \"granularity_time\",\n51 |       \"location_code\",\n52 |       \"date\",\n53 |       \"age\",\n54 |       \"sex\"\n55 |     ),\n56 |     censors = list(\n57 |       anon = list(\n58 | \n59 |       )\n60 |     ),\n61 |     validator_field_types = sc::validator_field_types_sykdomspulsen,\n62 |     validator_field_contents = sc::validator_field_contents_sykdomspulsen,\n63 |     info = \"This db table is used for...\"\n64 |   )\n\nChangelog\n2021-07-15: Draft created.\n\n\n\n",
      "last_modified": "2021-07-15T11:13:19+02:00"
    }
  ],
  "collections": []
}
